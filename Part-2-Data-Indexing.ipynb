{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86090e13",
   "metadata": {},
   "source": [
    "# Data & Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dc726c",
   "metadata": {},
   "source": [
    "## 1. Data Quality: The Foundation of RAG Systems\n",
    "\n",
    "Before diving into chunking strategies, it's crucial to understand that **clean, well-structured data is the foundation** of any successful RAG (Retrieval-Augmented Generation) system.\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Data Sources\n",
    "\n",
    "Your data can come from anywhere - the principles remain the same:\n",
    "\n",
    "**Common Data Sources:**\n",
    "- üìù **Logs** - Application logs, system logs, audit trails\n",
    "  - *Example*: Server error logs, user activity logs, API request logs\n",
    "- üìÑ **PDFs** - Documents, reports, manuals, research papers\n",
    "  - *Example*: Product documentation, scientific papers, legal contracts\n",
    "- üóÑÔ∏è **Document Stores** - Confluence, SharePoint, Google Docs\n",
    "  - *Example*: Internal wikis, company policies, team documentation\n",
    "- üõ†Ô∏è **Tools & APIs** - Jira, Slack, email archives\n",
    "  - *Example*: Support tickets, team conversations, customer emails\n",
    "- üíæ **Actual Databases** - SQL databases, NoSQL stores, data warehouses\n",
    "  - *Example*: Customer records, product catalogs, transaction histories\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Why Data Quality Matters\n",
    "\n",
    "**Garbage In = Garbage Out**: No matter how sophisticated your chunking strategy, embedding model, or vector database is, poor quality input data will lead to poor retrieval results and ultimately, poor AI responses.\n",
    "\n",
    "**Real-world Impact:**\n",
    "- **Noisy data** (e.g., HTML tags, formatting artifacts) ‚Üí Embeddings capture irrelevant features ‚Üí Wrong chunks retrieved\n",
    "- **Inconsistent formatting** (e.g., \"01/12/2024\" vs \"2024-01-12\") ‚Üí Model can't match related information\n",
    "- **Encoding errors** (e.g., \"don√¢‚Ç¨‚Ñ¢t\" instead of \"don't\") ‚Üí Semantic meaning lost ‚Üí Poor search results\n",
    "- **Duplicate content** ‚Üí Wastes vector storage space and returns redundant results\n",
    "\n",
    "**Example**: If your PDF has \"Pr√Øce: $100\" (bad OCR) instead of \"Price: $100\", a user searching for \"product price\" may not find this information because the embedding won't recognize \"Pr√Øce\" as related to \"price\".\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Key Data Quality Principles\n",
    "\n",
    "#### 1. **Remove Noise**\n",
    "\n",
    "- Strip out unnecessary HTML tags, special characters\n",
    "- Remove duplicate content, headers/footers that repeat\n",
    "- **Clean up OCR errors from scanned documents:**\n",
    "\n",
    "  **Common OCR Problems:**\n",
    "  - Misrecognized characters (e.g., '0' mistaken for 'O', '1' for 'l', 'rn' for 'm')\n",
    "  - Random symbols and artifacts (stray punctuation, page numbers in text)\n",
    "  - Incorrectly split words across lines\n",
    "  - Lost paragraph structure and formatting\n",
    "\n",
    "  **How to Fix OCR Errors:**\n",
    "  1. **Use spell checkers** - Tools like `textblob`, `pyspellchecker` to catch misrecognized words\n",
    "  2. **Find/Replace patterns** - Use regex to fix common OCR mistakes:\n",
    "     ```python\n",
    "     # Example: Replace common OCR character errors\n",
    "     text = text.replace('0', 'O').replace('rn', 'm')\n",
    "     ```\n",
    "\n",
    "#### 2. **Ensure Consistency**\n",
    "\n",
    "- **Standardize date formats, units, terminology**\n",
    "  - Dates: \"2024-01-15\" vs \"Jan 15, 2024\" vs \"15/01/24\" ‚Üí Choose one format\n",
    "  - Units: \"5kg\" vs \"5 kilograms\" vs \"5000g\" ‚Üí Normalize to consistent unit\n",
    "  - Terminology: \"customer\" vs \"client\" vs \"user\" ‚Üí Use consistent term\n",
    "  \n",
    "- **Fix spelling and grammatical errors**\n",
    "  - Use spell checkers: `textblob`, `pyspellchecker`, or language-specific tools\n",
    "  - Fix common typos that could confuse semantic search\n",
    "  \n",
    "- **Normalize text encoding (UTF-8)**\n",
    "  - **Why**: Different encodings (Latin-1, Windows-1252, UTF-16) represent characters differently\n",
    "  - **Problem**: \"caf√©\" in Latin-1 may appear as \"caf√É¬©\" when read as UTF-8 (mojibake)\n",
    "  - **Solution**: Detect source encoding and convert everything to UTF-8\n",
    "  - **Tools**: \n",
    "    - `chardet` library to detect encoding\n",
    "    - `ftfy` library to fix encoding errors\n",
    "    - `unicodedata.normalize()` for Unicode normalization (NFC/NFD/NFKC/NFKD)\n",
    "\n",
    "#### 3. **Preserve Structure**\n",
    "\n",
    "- Maintain logical document hierarchy (headings, sections)\n",
    "- Keep metadata (title, author, date, source)\n",
    "- Preserve context indicators (lists, tables, code blocks)\n",
    "\n",
    "---\n",
    "\n",
    "1 hour of data cleaning can prevent 10+ hours of troubleshooting why your RAG system returns irrelevant answers.\n",
    "\n",
    "---\n",
    "\n",
    "Now let's proceed with chunking strategies on clean data..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd0179a",
   "metadata": {},
   "source": [
    "## 2. Character-Based Chunking: Overview\n",
    "\n",
    "Character-based chunking splits text based on character count, using various strategies to find natural boundaries (paragraphs, sentences, words).\n",
    "\n",
    "We'll compare **3 LangChain approaches**:\n",
    "1. **RecursiveCharacterTextSplitter** - Multi-separator intelligence (RECOMMENDED)\n",
    "2. **CharacterTextSplitter** - Simple single separator\n",
    "3. **NLTKTextSplitter** - Sentence-aware splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7e10e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# APPROACH 1: RecursiveCharacterTextSplitter (RECOMMENDED)\n",
    "# üéØ NOTE: Same splitter with chunk_size=50 ‚Üí words, 300 ‚Üí sentences, 1000 ‚Üí paragraphs!\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"1Ô∏è‚É£ RecursiveCharacterTextSplitter - Multi-separator intelligence\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# --- Boundary logic for RecursiveCharacterTextSplitter ---\n",
    "#   - Tries to split at the most meaningful separator first \n",
    "#     (paragraph, then newline, then sentence, then space, then character)\n",
    "#   - Works down the list of separators, always ensuring the chunk does \n",
    "#     not exceed chunk_size\n",
    "#   - If a clean boundary (like a paragraph or sentence) would make the \n",
    "#     chunk too big, it tries the next separator\n",
    "#   - chunk_size is a strict upper limit: no chunk exceeds this value \n",
    "#     unless a single text unit (e.g., a paragraph) is longer than \n",
    "#     chunk_size (then it's split at the next available separator or \n",
    "#     hard limit)\n",
    "#   - If the separators list is not given or is empty, the splitter will \n",
    "#     default to splitting at every character (i.e., no semantic boundaries, \n",
    "#     just raw character chunks of chunk_size)\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sample_data import SAMPLE_TEXT  # Import here for independent execution\n",
    "\n",
    "# Test parameters\n",
    "test_chunk_size = 100  # üéØ This controls what we get!\n",
    "test_overlap = 20\n",
    "\n",
    "print(f\"\\nüìù Test text: {len(SAMPLE_TEXT)} characters\\n\")\n",
    "\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=test_chunk_size,\n",
    "    chunk_overlap=test_overlap,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Tries these in order\n",
    ")\n",
    "\n",
    "recursive_chunks = recursive_splitter.split_text(SAMPLE_TEXT)\n",
    "print(f\"‚úÖ Created {len(recursive_chunks)} chunks\")\n",
    "print(\"\\nüìä How it works: Tries multiple separators in priority order\")\n",
    "print(\"   Pros: Preserves semantic boundaries, intelligent splitting\")\n",
    "print(\"   Cons: None - this is the recommended approach\\n\")\n",
    "\n",
    "print(\"First 4 chunks:\")\n",
    "for i, chunk in enumerate(recursive_chunks[:6], 1):\n",
    "    print(f\"\\nChunk {i} ({len(chunk)} chars):\")\n",
    "    print(f\"'{chunk[:200]}{'...' if len(chunk) > 200 else ''}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12217dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# APPROACH 2: CharacterTextSplitter - Simple single separator\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"2Ô∏è‚É£ CharacterTextSplitter - Single separator approach\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# --- Boundary logic for CharacterTextSplitter ---\n",
    "#   - Splits only at the specified separator (e.g., space or newline)\n",
    "#   - Builds chunks by adding text segments until the NEXT separator would exceed chunk_size\n",
    "#   - When adding the next segment would go over the limit, it:\n",
    "#     ‚Üí Finishes the current chunk at the LAST separator that fit\n",
    "#     ‚Üí Starts a new chunk from that point\n",
    "#   - Example: chunk_size=100, separator=\" \"\n",
    "#     ‚Üí Text: \"word1 word2 word3 word4...\" (each word ~25 chars)\n",
    "#     ‚Üí Adds: \"word1 word2 word3 word4\" (100 chars) ‚úì Stops here (next word would exceed 100)\n",
    "#     ‚Üí Next separator (space before word5) is AFTER position 100, so chunk ends at word4\n",
    "#\n",
    "# ‚ö†Ô∏è CRITICAL: chunk_size CAN be exceeded (but only in rare cases):\n",
    "#   - If a SINGLE segment between separators is longer than chunk_size\n",
    "#   - Example: chunk_size=100, separator=\" \", text has a 150-character \n",
    "#     URL or long word (no spaces)\n",
    "#   - Result: That 150-char segment becomes its own chunk, EXCEEDING \n",
    "#     the 100-char limit\n",
    "#   - Why? The splitter can't break it without finding a separator\n",
    "#\n",
    "#   - If separator is empty string, will apply the chunk size as hard limit.\n",
    "\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from sample_data import SAMPLE_TEXT  # Import here for independent execution\n",
    "\n",
    "# Test parameters (same as RecursiveCharacterTextSplitter for comparison)\n",
    "test_chunk_size = 100\n",
    "test_overlap = 20\n",
    "\n",
    "print(f\"\\nüìù Test text: {len(SAMPLE_TEXT)} characters\\n\")\n",
    "\n",
    "character_splitter = CharacterTextSplitter(\n",
    "    separator=\" \",  # Split at spaces\n",
    "    chunk_size=test_chunk_size,\n",
    "    chunk_overlap=test_overlap,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "character_chunks = character_splitter.split_text(SAMPLE_TEXT)\n",
    "print(f\"‚úÖ Created {len(character_chunks)} chunks\")\n",
    "print(\"\\nüìä How it works: Splits only at the specified separator (space)\")\n",
    "print(\"   Pros: Simple, predictable\")\n",
    "print(\"   Cons: May split in the middle of sentences or paragraphs\")\n",
    "print(\"   Note: All chunks ‚â§100 chars because SAMPLE_TEXT has no words >100 chars!\\n\")\n",
    "\n",
    "print(\"First 4 chunks:\")\n",
    "for i, chunk in enumerate(character_chunks[:6], 1):\n",
    "    print(f\"\\nChunk {i} ({len(chunk)} chars):\")\n",
    "    print(f\"'{chunk[:200]}{'...' if len(chunk) > 200 else ''}'\")\n",
    "\n",
    "# üß™ DEMONSTRATION: What happens with a segment longer than chunk_size?\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"üß™ EDGE CASE DEMO: Text with a 120-character 'word' (URL)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Create test text with a long URL (no spaces, 120+ chars)\n",
    "edge_case_text = \"Normal text here. https://www.example-website-with-a-very-long-domain-name-that-exceeds-one-hundred-characters-to-demonstrate-the-edge-case.com/path More normal text after.\"\n",
    "\n",
    "edge_splitter = CharacterTextSplitter(separator=\" \", chunk_size=100, chunk_overlap=0, length_function=len)\n",
    "edge_chunks = edge_splitter.split_text(edge_case_text)\n",
    "\n",
    "print(f\"\\nCreated {len(edge_chunks)} chunks from edge case text:\")\n",
    "for i, chunk in enumerate(edge_chunks, 1):\n",
    "    exceeds = \"‚ö†Ô∏è EXCEEDS LIMIT!\" if len(chunk) > 100 else \"‚úì Within limit\"\n",
    "    print(f\"\\nChunk {i}: {len(chunk)} chars {exceeds}\")\n",
    "    print(f\"'{chunk}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b949b292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# APPROACH 3: NLTKTextSplitter - Sentence-aware splitting\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"3Ô∏è‚É£ NLTKTextSplitter - Sentence-aware approach\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# --- Boundary logic for NLTKTextSplitter ---\n",
    "#   - Splits at sentence boundaries using NLTK's sentence tokenizer\n",
    "#   - Merges sentences into a chunk until adding another would exceed \n",
    "#     chunk_size\n",
    "#   - If a single sentence is longer than chunk_size, it will be its own \n",
    "#     chunk (and may greatly exceed chunk_size; this is common if your \n",
    "#     text has long sentences)\n",
    "#   - The chunk_size is a soft limit: it will not break up sentences to \n",
    "#     enforce the limit\n",
    "\n",
    "from langchain_text_splitters import NLTKTextSplitter\n",
    "from sample_data import SAMPLE_TEXT  # Import here for independent execution\n",
    "\n",
    "# Test parameters (same as others for comparison)\n",
    "test_chunk_size = 100\n",
    "test_overlap = 20\n",
    "\n",
    "print(f\"\\nüìù Test text: {len(SAMPLE_TEXT)} characters\\n\")\n",
    "\n",
    "nltk_splitter = NLTKTextSplitter(\n",
    "    chunk_size=test_chunk_size,\n",
    "    chunk_overlap=test_overlap,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "nltk_chunks = nltk_splitter.split_text(SAMPLE_TEXT)\n",
    "print(\"\\nüìä How it works: Splits at sentence boundaries using NLTK\")\n",
    "print(\"   Pros: Respects sentence boundaries, good for natural language\")\n",
    "print(\"   Cons: Sentences longer than chunk_size will be their own chunk,\")\n",
    "print(\"         so chunk_size is not a hard limit\\n\")\n",
    "\n",
    "print(\"First 4 chunks:\")\n",
    "for i, chunk in enumerate(nltk_chunks[:4], 1):\n",
    "    print(f\"\\nChunk {i} ({len(chunk)} chars):\")\n",
    "    print(f\"'{chunk[:200]}{'...' if len(chunk) > 200 else ''}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üí° COMPARISON SUMMARY:\")\n",
    "print(\"   ‚Ä¢ RecursiveCharacterTextSplitter: Best balance (RECOMMENDED)\")\n",
    "print(\"   ‚Ä¢ CharacterTextSplitter: Simplest, but may break semantic units\")\n",
    "print(\"   ‚Ä¢ NLTKTextSplitter: Best for preserving complete sentences,\")\n",
    "print(\"                       but chunk_size is a soft limit\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7056330",
   "metadata": {},
   "source": [
    "## 7. Semantic/Topic-Based Chunking\n",
    "\n",
    "True semantic chunking that groups sentences based on **semantic similarity** rather than just paragraph boundaries.\n",
    "\n",
    "**How it works:**\n",
    "1. Splits text into sentences\n",
    "2. Calculates semantic similarity between consecutive sentences using TF-IDF vectors\n",
    "3. Creates chunk boundaries where similarity drops below a threshold\n",
    "4. Adds configurable overlap to preserve context\n",
    "\n",
    "**Term Frequency-Inverse Document Frequency**, is a numerical statistic used in information retrieval and text mining to evaluate the importance of a word in a document within a larger collection of documents.\n",
    "    \n",
    "**Why overlap matters:** Retrieved chunks may need context from semantically related preceding content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c72981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and download NLTK only where needed (for semantic chunking)\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def chunk_by_semantic_similarity(text: str, similarity_threshold: float = 0.5, overlap_sentences: int = 2, min_chunk_size: int = 2) -> list:\n",
    "    \"\"\"\n",
    "    Semantic chunking based on sentence similarity using TF-IDF vectors\n",
    "    \n",
    "    Parameters:\n",
    "    - similarity_threshold: Cosine similarity threshold (0-1). Lower = more chunks\n",
    "    - overlap_sentences: Number of sentences from previous chunk to include as context\n",
    "    - min_chunk_size: Minimum number of sentences per chunk\n",
    "    \n",
    "    Groups semantically similar sentences together and creates boundaries where\n",
    "    the semantic topic shifts (similarity drops below threshold).\n",
    "    \n",
    "    Note: TF-IDF stands for Term Frequency-Inverse Document Frequency, a statistical measure used to evaluate how important a word is to a sentence or document.\n",
    "    \"\"\"\n",
    "    # STEP 1: Split the text into individual sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    if len(sentences) <= min_chunk_size:\n",
    "        return [text]\n",
    "    \n",
    "    # STEP 2: Convert sentences to numerical vectors using TF-IDF\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    try:\n",
    "        sentence_vectors = vectorizer.fit_transform(sentences)\n",
    "    except ValueError:\n",
    "        return [text]\n",
    "    \n",
    "    # STEP 3: Calculate similarity between consecutive sentences\n",
    "    # similarities contains the cosine similarity between each pair of consecutive sentences \n",
    "    # (i and i+1) in the text.\n",
    "    # Bigger cosine values (closer to 1) mean lower angle and hence more similar ; \n",
    "    # smaller cosine values (closer to 0) mean bigger angle and hence less similar.\n",
    "    # Cosine 0=1\n",
    "    # Cosine 90 = 0\n",
    "    # Cosine 180 = -1\n",
    "    similarities = [cosine_similarity(sentence_vectors[i:i+1], sentence_vectors[i+1:i+2])[0][0] for i in range(len(sentences) - 1)]\n",
    "    \n",
    "    # STEP 4: Identify chunk boundaries where topics change\n",
    "    # The similarities list contains similarity scores between consecutive sentences:\n",
    "    # similarities[i] = similarity between sentence[i] and sentence[i+1]\n",
    "    # \n",
    "    # When similarity[i] < threshold, it means sentence[i] and sentence[i+1] are \n",
    "    # dissimilar (topic shift), so we create a boundary BETWEEN them.\n",
    "    # \n",
    "    # Logic:\n",
    "    # - Start with sentence 0 in chunk 0\n",
    "    # - For each similarity score, check if it's below threshold\n",
    "    # - If yes AND we have enough sentences in current chunk (>= min_chunk_size):\n",
    "    #   ‚Üí Create new chunk starting at sentence[i+1]\n",
    "    #   ‚Üí Reset counter to 1 (because sentence[i+1] is first in new chunk)\n",
    "    # - Otherwise, keep adding sentences to current chunk\n",
    "    chunk_boundaries = [0]  # First chunk starts at sentence 0\n",
    "    current_chunk_size = 1  # We start with 1 sentence in the chunk\n",
    "    \n",
    "    for i, sim in enumerate(similarities):\n",
    "        # At this point, we've seen i+1 sentences (0 to i inclusive)\n",
    "        # similarities[i] tells us if sentence[i] and sentence[i+1] are similar\n",
    "        \n",
    "        if sim < similarity_threshold and current_chunk_size >= min_chunk_size:\n",
    "            # Topic shift detected! Start new chunk at sentence i+1\n",
    "            chunk_boundaries.append(i + 1)\n",
    "            current_chunk_size = 1  # New chunk starts with 1 sentence\n",
    "        else:\n",
    "            # No topic shift, continue adding to current chunk\n",
    "            current_chunk_size += 1\n",
    "    \n",
    "    # Ensure last boundary is at the end\n",
    "    if chunk_boundaries[-1] != len(sentences):\n",
    "        chunk_boundaries.append(len(sentences))\n",
    "    \n",
    "    # STEP 5: Create actual text chunks with optional overlap\n",
    "    # chunk_boundaries contains indices like [0, 5, 10, 15] meaning:\n",
    "    # - Chunk 0: sentences[0:5] (sentences 0,1,2,3,4)\n",
    "    # - Chunk 1: sentences[5:10] (sentences 5,6,7,8,9)\n",
    "    # - Chunk 2: sentences[10:15] (sentences 10,11,12,13,14)\n",
    "    #\n",
    "    # With overlap_sentences=2:\n",
    "    # - Chunk 0: sentences[0:5] (no overlap for first chunk)\n",
    "    # - Chunk 1: sentences[3:10] (includes 2 sentences from previous chunk)\n",
    "    # - Chunk 2: sentences[8:15] (includes 2 sentences from previous chunk)\n",
    "    #\n",
    "    # This overlap preserves context across chunk boundaries!\n",
    "    \n",
    "    chunks = []\n",
    "    for i in range(len(chunk_boundaries) - 1):\n",
    "        start_idx = chunk_boundaries[i]\n",
    "        end_idx = chunk_boundaries[i + 1]\n",
    "        \n",
    "        # Add overlap from previous chunk (except for first chunk)\n",
    "        if i > 0 and overlap_sentences > 0:\n",
    "            overlap_start = max(0, start_idx - overlap_sentences)\n",
    "            chunk_sentences = sentences[overlap_start:end_idx]\n",
    "        else:\n",
    "            # First chunk has no previous context to overlap\n",
    "            chunk_sentences = sentences[start_idx:end_idx]\n",
    "        \n",
    "        chunk = \" \".join(chunk_sentences)\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Test semantic chunking\n",
    "from sample_data import SAMPLE_TEXT  # Import here for independent execution\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SEMANTIC CHUNKING DEMONSTRATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. Semantic chunking with HIGH threshold (0.7) - more, smaller chunks:\")\n",
    "print(\"   (Creates new chunk even for moderate topic shifts; more sensitive, so more chunks)\")\n",
    "semantic_chunks_high = chunk_by_semantic_similarity(SAMPLE_TEXT, similarity_threshold=0.10, overlap_sentences=2)\n",
    "print(f\"‚úÖ Created {len(semantic_chunks_high)} chunks\\n\")\n",
    "for i, chunk in enumerate(semantic_chunks_high[:2]):\n",
    "    print(f\"Chunk {i+1} (length: {len(chunk)} chars):\")\n",
    "    print(f\"{chunk[:250]}...\\n\")\n",
    "\n",
    "print(\"\\n2. Semantic chunking with LOW threshold (0.3) - fewer, larger chunks:\")\n",
    "print(\"   (Only creates new chunk when sentences are very different; less sensitive, so fewer chunks)\")\n",
    "semantic_chunks_low = chunk_by_semantic_similarity(SAMPLE_TEXT, similarity_threshold=0.3, overlap_sentences=2)\n",
    "print(f\"‚úÖ Created {len(semantic_chunks_low)} chunks\\n\")\n",
    "for i, chunk in enumerate(semantic_chunks_low[:2]):\n",
    "    print(f\"Chunk {i+1} (length: {len(chunk)} chars):\")\n",
    "    print(f\"{chunk[:250]}...\\n\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Key Insight: Lower threshold = less sensitive to topic changes = fewer, larger chunks\")\n",
    "print(\"Overlap preserves semantic context across chunk boundaries!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5589862a",
   "metadata": {},
   "source": [
    "## 7.0 Token-Based Chunking\n",
    "\n",
    "Token-based chunking splits text into chunks based on the number of tokens (not characters). This is especially important for LLMs, as most models have token limits and count tokens differently than characters or words.\n",
    "\n",
    "**What are tokens**\n",
    "Tokens are not always words; they can be word pieces or punctuation.\n",
    "LLMs process and count input/output in tokens, not characters or words.\n",
    "Token limits (e.g., 4096 tokens for GPT-3.5) determine how much text the model can handle at once.\n",
    "Example:\n",
    "\"cat\" = 1 token\n",
    "\"unbelievable\" = 3 tokens (\"un\", \"believ\", \"able\")\n",
    "\"Hello, world!\" = 4 tokens (\"Hello\", \",\", \"world\", \"!\")\n",
    "\n",
    "**Key points:**\n",
    "- A token is a unit of text as defined by the tokenizer (e.g., a word, part of a word, or punctuation) this is LLM specific.\n",
    "- Token-based chunking ensures each chunk fits within the model's context window. (In Augmentation Layer for cost optimization)\n",
    "- Common tokenizers: `tiktoken` (OpenAI), Hugging Face tokenizers, etc.\n",
    "- Useful for preparing data for LLMs like GPT-3/4, which have strict token limits.\n",
    "\n",
    "Below is a demonstration using the `tiktoken` tokenizer (used by OpenAI models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0dd394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKEN-BASED CHUNKING DEMONSTRATION (using LangChain's TokenTextSplitter)\n",
    "# ============================================================================\n",
    "# üéØ KEY DIFFERENCE: chunk_size is in TOKENS, not characters!\n",
    "# ============================================================================\n",
    "\n",
    "try:\n",
    "    from langchain_text_splitters import TokenTextSplitter\n",
    "except ImportError:\n",
    "    print(\"‚ùå langchain_text_splitters not installed. Run: pip install langchain-text-splitters\")\n",
    "    TokenTextSplitter = None\n",
    "\n",
    "# --- Boundary logic for token-based chunking (LangChain) ---\n",
    "#   - Splits text so that each chunk contains at most `chunk_size` tokens \n",
    "#     (as defined by the tokenizer)\n",
    "#   - Overlap can be specified in tokens, not characters\n",
    "#   - Uses tiktoken under the hood for OpenAI models (encoding_name param)\n",
    "#   - Ensures compatibility with LLM context windows \n",
    "#     (e.g., 4096 tokens for GPT-3.5, 8k/32k for GPT-4)\n",
    "#   - If langchain_text_splitters is not available, this cell will not run\n",
    "#\n",
    "# üí° WHY TOKEN-BASED?\n",
    "#   - LLMs count input in tokens, not characters\n",
    "#   - Guarantees chunks fit within model limits \n",
    "#     (e.g., GPT-3.5's 4096 token limit)\n",
    "#   - Predictable API costs (OpenAI charges per token)\n",
    "#   - Character count varies: 100 tokens might be 200-500 chars \n",
    "#     depending on word complexity\n",
    "#\n",
    "# üìä TOKEN vs CHARACTER EXAMPLE:\n",
    "#  A token is a statistical unit that the model learned to recognize as \n",
    "#  a common text pattern, regardless of whether it has standalone meaning. \n",
    "#  This is specific to LLM.\n",
    "#   - \"cat\" = 1 token, 3 characters\n",
    "#   - \"unbelievable\" = 3 tokens (un + believ + able), 12 characters\n",
    "#   - Same character count can have different token counts!\n",
    "\n",
    "# Example usage with LangChain's TokenTextSplitter\n",
    "if TokenTextSplitter is not None:\n",
    "    from sample_data import SAMPLE_TEXT  # Import here for independent execution\n",
    "    \n",
    "    # Configure token-based chunking parameters\n",
    "    token_chunk_size = 100    # Maximum 100 TOKENS per chunk (not characters!)\n",
    "    token_chunk_overlap = 20    # 20 TOKEN overlap between chunks (for context preservation)\n",
    "    \n",
    "    # Initialize the token splitter with tiktoken encoding for OpenAI models\n",
    "    token_splitter = TokenTextSplitter(\n",
    "        chunk_size=token_chunk_size,        # Limit: 100 tokens per chunk\n",
    "        chunk_overlap=token_chunk_overlap,  # Overlap: 20 tokens between chunks\n",
    "        model_name=\"gpt-3.5-turbo\"          # Use GPT-3.5-turbo's tokenizer (tiktoken), OpenAI's official fast tokenizer library,\n",
    "        # When you installed langchain-text-splitters, it automatically installed tiktoken as a dependency.\n",
    "        # Note: Use model_name for model identifiers (gpt-3.5-turbo, gpt-4), \n",
    "        # or encoding_name for encoding identifiers (cl100k_base, p50k_base, r50k_base)\n",
    "    )\n",
    "    \n",
    "    # Split the sample text into token-based chunks\n",
    "    token_chunks = token_splitter.split_text(SAMPLE_TEXT)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"‚úÖ Created {len(token_chunks)} token-based chunks \" +\n",
    "          f\"(chunk_size={token_chunk_size}, overlap={token_chunk_overlap})\")\n",
    "    print(f\"   Note: Each chunk has ‚â§{token_chunk_size} tokens, \" +\n",
    "          \"but character count varies!\\n\")\n",
    "    \n",
    "    # Show first 3 chunks with both character and token counts\n",
    "    for i, chunk in enumerate(token_chunks[:6], 1):\n",
    "        char_count = len(chunk)\n",
    "        token_count = len(token_splitter._tokenizer.encode(chunk))\n",
    "        print(f\"\\nToken Chunk {i}:\")\n",
    "        print(f\"  - Character count: {char_count} chars\")\n",
    "        print(f\"  - Token count: {token_count} tokens (‚â§{token_chunk_size})\")\n",
    "        print(f\"  - Content preview: {chunk[:200]}...\")\n",
    "        \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üí° OBSERVATION: Character count varies, but token count is \" +\n",
    "          \"controlled!\")\n",
    "    print(\"   This ensures chunks always fit within LLM context windows.\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761ec88c",
   "metadata": {},
   "source": [
    "## 7.1 Store Chunks in Vector Database\n",
    "\n",
    "Now let's take the semantic chunks and store them in a **vector store** with embeddings.\n",
    "**Steps:**\n",
    "1. Chunk the text (already done above)\n",
    "2. Generate embeddings using an embedding model\n",
    "3. Store embeddings in a vector database (ChromaDB)\n",
    "4. Query to retrieve similar chunks\n",
    "\n",
    "**Note:** This requires `chromadb` and `sentence-transformers` packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aec0792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's check if required packages are installed\n",
    "try:\n",
    "    import chromadb\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    print(\"‚úÖ Required packages already installed!\")\n",
    "except ImportError as e:\n",
    "    print(\"‚ùå Missing packages. Installing...\")\n",
    "    print(\"Run: pip install chromadb sentence-transformers\")\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Import dependencies for independent execution\n",
    "from sample_data import SAMPLE_TEXT\n",
    "# Note: chunk_by_semantic_similarity function must be defined before running this cell\n",
    "# (either by running the semantic chunking cell first, or by importing from a module)\n",
    "\n",
    "# Use the semantic chunks we created earlier with threshold=0.5\n",
    "chunks_to_embed = chunk_by_semantic_similarity(SAMPLE_TEXT, similarity_threshold=0.15, overlap_sentences=2)\n",
    "print(f\"\\nüì¶ Using {len(chunks_to_embed)} semantic chunks for embedding\")\n",
    "\n",
    "# STEP 1: Initialize embedding model\n",
    "# üìä DIMENSIONALITY: Different models produce different vector sizes\n",
    "# Popular embedding models and their dimensions:\n",
    "# - all-MiniLM-L6-v2: 384d (FAST, lightweight, good for most cases) ‚ö° [USING THIS]\n",
    "# - all-mpnet-base-v2: 768d (Better quality, more compute)\n",
    "# - sentence-t5-base: 768d (T5-based, good for semantic search)\n",
    "# - paraphrase-multilingual: 768d (Supports 50+ languages)\n",
    "# - text-embedding-ada-002 (OpenAI): 1536d (Best quality, API cost)\n",
    "# - text-embedding-3-small (OpenAI): 1536d (Improved ada-002)\n",
    "# - text-embedding-3-large (OpenAI): 3072d (Highest quality, expensive)\n",
    "# - voyage-large-2 (Voyage AI): 1536d (High-quality commercial API)\n",
    "# - cohere-embed-v3 (Cohere): 1024d (Multilingual, commercial)\n",
    "# \n",
    "# Trade-off: Higher dimensions = better semantic capture but slower search & more storage\n",
    "print(\"\\nüîÑ Loading embedding model (this may take a moment)...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')  # 384-dimensional embeddings\n",
    "print(\"‚úÖ Embedding model loaded!\")\n",
    "\n",
    "# STEP 2: Generate embeddings for all chunks\n",
    "# Each chunk becomes a 384-dimensional vector representing its meaning\n",
    "print(f\"\\nüîÑ Generating embeddings for {len(chunks_to_embed)} chunks...\")\n",
    "embeddings = embedding_model.encode(chunks_to_embed, show_progress_bar=True)\n",
    "print(f\"‚úÖ Generated {len(embeddings)} embeddings, each with {embeddings[0].shape[0]} dimensions\")\n",
    "\n",
    "# STEP 3: Initialize ChromaDB (vector store)\n",
    "# ChromaDB stores embeddings persistently and enables fast similarity search\n",
    "print(\"\\nüîÑ Initializing ChromaDB vector store...\")\n",
    "chroma_client = chromadb.Client()  # In-memory database (for demo)\n",
    "\n",
    "# üìê SIMILARITY METRIC FOR VECTOR SEARCH: How we measure \"closeness\" between embeddings\n",
    "# This is CRITICAL for retrieval quality - different from chunking similarity!\n",
    "#\n",
    "# Available options in ChromaDB: {\"hnsw:space\": \"cosine\" | \"l2\" | \"ip\"}\n",
    "#\n",
    "# 1. COSINE SIMILARITY (RECOMMENDED) ‚úì [USING THIS]\n",
    "#    - Measures: Angle between vectors (0¬∞ = identical, 90¬∞ = unrelated, 180¬∞ = opposite)\n",
    "#    - Range: -1 to 1 (for embeddings, typically 0 to 1)\n",
    "#    - Formula: dot(A,B) / (||A|| * ||B||)\n",
    "#    - When to use: DEFAULT choice for semantic search with embeddings\n",
    "#    - Why best: Direction matters, not magnitude - normalized automatically\n",
    "#    - Best for: RAG, semantic search, Q&A systems, document similarity\n",
    "#\n",
    "# 2. L2 (EUCLIDEAN DISTANCE)\n",
    "#    - Measures: Straight-line distance in n-dimensional space\n",
    "#    - Range: 0 to infinity (0 = identical, larger = more different)\n",
    "#    - Formula: sqrt(sum((A[i] - B[i])¬≤))\n",
    "#    - When to use: When vector magnitudes are meaningful and normalized\n",
    "#    - Caveat: Sensitive to scale - embeddings with larger magnitudes rank higher\n",
    "#    - Best for: Image embeddings, when using normalized embeddings only\n",
    "#\n",
    "# 3. IP (INNER PRODUCT / DOT PRODUCT)\n",
    "#    - Measures: Both angle AND magnitude (unnormalized cosine)\n",
    "#    - Range: -infinity to infinity\n",
    "#    - Formula: sum(A[i] * B[i])\n",
    "#    - When to use: ONLY when embeddings are pre-normalized (||A|| = ||B|| = 1)\n",
    "#    - Why faster: Skips division step from cosine calculation\n",
    "#    - Performance: ~10-20% faster than cosine for large-scale systems\n",
    "#    - Best for: Production systems with normalized embeddings (OpenAI, Cohere)\n",
    "#    - Caveat: If not normalized, longer vectors artificially rank higher!\n",
    "#\n",
    "# üí° KEY DECISION FACTORS:\n",
    "#    - Embedding Model: Most modern models (sentence-transformers, OpenAI) are normalized ‚Üí use COSINE or IP\n",
    "#    - Scale: Millions of vectors? Use IP for speed (if normalized)\n",
    "#    - Safety: Unsure if normalized? Use COSINE (always safe)\n",
    "#    - Legacy systems: L2 if that's what you've always used (consistency matters)\n",
    "#\n",
    "# üéØ RECOMMENDATION FOR RAG:\n",
    "#    - Start with COSINE (most intuitive, always correct)\n",
    "#    - Switch to IP only if: (1) embeddings are normalized, (2) need extra speed at scale\n",
    "#\n",
    "# üìä PERFORMANCE COMPARISON (1M vectors):\n",
    "#    - Cosine: ~5ms per query\n",
    "#    - L2: ~5ms per query  \n",
    "#    - IP: ~4ms per query (fastest, but needs normalized vectors)\n",
    "#\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"hr_policy_chunks\",\n",
    "    metadata={\n",
    "        \"hnsw:space\": \"cosine\",  # Using cosine similarity - safest & most interpretable\n",
    "        \"description\": \"Semantic chunks from HR Remote Work Policy\"\n",
    "    }\n",
    ")\n",
    "print(\"‚úÖ ChromaDB collection created with COSINE similarity!\")\n",
    "\n",
    "# STEP 4: Add chunks with embeddings to vector store\n",
    "# Each chunk is stored with:\n",
    "# - id: unique identifier\n",
    "# - embedding: the vector representation\n",
    "# - document: the original text\n",
    "# - metadata: additional info (chunk index, length, etc.)\n",
    "print(f\"\\nüîÑ Storing {len(chunks_to_embed)} chunks in vector database...\")\n",
    "collection.add(\n",
    "    ids=[f\"chunk_{i}\" for i in range(len(chunks_to_embed))],  # Unique IDs\n",
    "    embeddings=embeddings.tolist(),  # Vector representations\n",
    "    documents=chunks_to_embed,  # original text for retrieval alongside embeddings\n",
    "    metadatas=[{\"chunk_index\": i, \"length\": len(chunk)} for i, chunk in enumerate(chunks_to_embed)]\n",
    ")\n",
    "print(\"‚úÖ All chunks stored in vector database!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VECTOR STORE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"‚Ä¢ Total chunks stored: {collection.count()}\")\n",
    "print(f\"‚Ä¢ Embedding dimensions: {embeddings[0].shape[0]}\")\n",
    "print(f\"‚Ä¢ Embedding model: all-MiniLM-L6-v2 (sentence-transformers)\")\n",
    "print(f\"‚Ä¢ Vector store: ChromaDB (in-memory)\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e832f4f",
   "metadata": {},
   "source": [
    "## 7.2 Query the Vector Store: Vector & Hybrid Search\n",
    "\n",
    "Now let's demonstrate retrieval methods:\n",
    "1. **Vector Search**: Pure semantic similarity using embeddings\n",
    "2. **Hybrid Search**: Combines keyword matching (BM25) + vector similarity for best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bb23ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query: Ask about technology requirements\n",
    "query = \"What are the internet speed requirements for remote work?\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SEMANTIC SEARCH & HYBRID RETRIEVAL DEMONSTRATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüîç Query: '{query}'\")\n",
    "\n",
    "# STEP 1: Convert query to embedding using the same model\n",
    "query_embedding = embedding_model.encode([query])[0]\n",
    "print(f\"‚úÖ Query converted to {len(query_embedding)}-dimensional vector\")\n",
    "\n",
    "# ============================================================================\n",
    "# VECTOR SEARCH: Semantic similarity using embeddings\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"1Ô∏è‚É£ VECTOR SEARCH (Semantic Similarity)\")\n",
    "print(\"-\" * 80)\n",
    "# ChromaDB uses HNSW (Hierarchical Navigable Small World) index\n",
    "# This enables sub-millisecond search across billions of vectors\n",
    "results_vector = collection.query(\n",
    "    query_embeddings=[query_embedding.tolist()],\n",
    "    n_results=3  # Retrieve top 3 most relevant chunks\n",
    ")\n",
    "\n",
    "print(\"Top 3 results by semantic similarity:\\n\")\n",
    "for i, (doc, metadata, distance) in enumerate(zip(\n",
    "    results_vector['documents'][0], \n",
    "    results_vector['metadatas'][0],\n",
    "    results_vector['distances'][0]\n",
    "), 1):\n",
    "    similarity_score = 1 - distance  # Convert distance to similarity\n",
    "    print(f\"Rank {i} | Similarity: {similarity_score:.4f}\")\n",
    "    print(f\"Content: {doc[:150]}...\")\n",
    "    print()\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"üí° Vector search finds semantically similar chunks using embeddings\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31164579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HYBRID SEARCH: Keyword (BM25) + Vector Re-ranking\n",
    "# ============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"2Ô∏è‚É£ HYBRID SEARCH (Keyword + Vector Re-ranking)\")\n",
    "print(\"=\" * 80)\n",
    "print(\"üí° Combines BM25 keyword matching with semantic vector search\")\n",
    "print(\"   Strategy: Vector search finds top candidates ‚Üí BM25 re-ranks them\\n\")\n",
    "\n",
    "try:\n",
    "    from rank_bm25 import BM25Okapi\n",
    "    import numpy as np\n",
    "    \n",
    "    # ========================================================================\n",
    "    # IMPORTANT: We are NOT creating a second vector store here!\n",
    "    # ========================================================================\n",
    "    # BM25 is a KEYWORD-BASED ranking algorithm (like classic search engines).\n",
    "    # It does NOT use vectors or embeddings. Instead, it:\n",
    "    #   1. Builds an inverted index (word ‚Üí documents mapping)\n",
    "    #   2. Scores documents based on keyword frequency and rarity (TF-IDF-like)\n",
    "    #   3. Returns relevance scores for keyword matches\n",
    "    #\n",
    "    # We're combining TWO different retrieval systems:\n",
    "    #   ‚Ä¢ ChromaDB (vector store) ‚Üí Semantic similarity via embeddings\n",
    "    #   ‚Ä¢ BM25 (keyword index) ‚Üí Exact keyword matching via statistical ranking\n",
    "    #\n",
    "    # This \"hybrid\" approach gives us BOTH semantic understanding AND exact matches!\n",
    "    # ========================================================================\n",
    "    \n",
    "    # STEP 1: Create BM25 keyword index (NOT a vector store!)\n",
    "    # This builds an inverted index for fast keyword-based retrieval\n",
    "    tokenized_chunks = [chunk.lower().split() for chunk in chunks_to_embed]\n",
    "    bm25 = BM25Okapi(tokenized_chunks)  # BM25 index created from same chunks\n",
    "    \n",
    "    # STEP 2: Get vector similarity scores from the earlier vector search\n",
    "    # ========================================================================\n",
    "    # Reuse results_vector from the earlier query (top 3 chunks)\n",
    "    # ========================================================================\n",
    "    # We already queried ChromaDB above and got the top 3 chunks with their\n",
    "    # distances. Now we'll extract those and convert to similarity scores.\n",
    "    # ========================================================================\n",
    "    \n",
    "    # Extract chunk indices and distances from results_vector\n",
    "    top_chunk_ids = results_vector['ids'][0]\n",
    "    top_distances = results_vector['distances'][0]\n",
    "    \n",
    "    # Convert to similarity scores (1 - distance) and map to chunk indices\n",
    "    vector_scores_dict = {}\n",
    "    for chunk_id, distance in zip(top_chunk_ids, top_distances):\n",
    "        chunk_idx = int(chunk_id.split('_')[1])  # Extract index from \"chunk_0\", \"chunk_1\", etc.\n",
    "        vector_scores_dict[chunk_idx] = 1 - distance  # Convert distance to similarity\n",
    "    \n",
    "    # STEP 3: Get BM25 keyword scores for the top 3 chunks only\n",
    "    # BM25 scores how well each of the top 3 chunks matches the query keywords\n",
    "    query_tokens = query.lower().split()\n",
    "    \n",
    "    # Get BM25 scores only for the top 3 chunks\n",
    "    bm25_scores_dict = {}\n",
    "    for chunk_idx in vector_scores_dict.keys():\n",
    "        # Score this specific chunk\n",
    "        chunk_tokens = tokenized_chunks[chunk_idx]\n",
    "        # BM25 score for single document\n",
    "        bm25_score = bm25.get_scores(query_tokens)[chunk_idx]\n",
    "        bm25_scores_dict[chunk_idx] = bm25_score\n",
    "    \n",
    "    # ========================================================================\n",
    "    # BM25 SCORE NORMALIZATION: Scale scores to 0-1 range for fair comparison\n",
    "    # ========================================================================\n",
    "    # WHY NORMALIZE?\n",
    "    # - BM25 raw scores are unbounded (can be any positive number)\n",
    "    # - Vector similarity scores are already in 0-1 range (from cosine similarity)\n",
    "    # - We need BOTH scores on the same scale to combine them fairly\n",
    "    #\n",
    "    # HOW IT WORKS (Min-Max Normalization):\n",
    "    # Formula: normalized = (score - min) / (max - min)\n",
    "    # \n",
    "    # Example: If BM25 scores are [0.5, 2.0, 5.0]\n",
    "    #   - min = 0.5, max = 5.0, range = 4.5\n",
    "    #   - Chunk 0: (0.5 - 0.5) / 4.5 = 0.0    (worst match)\n",
    "    #   - Chunk 1: (2.0 - 0.5) / 4.5 = 0.33   (medium match)\n",
    "    #   - Chunk 2: (5.0 - 0.5) / 4.5 = 1.0    (best match)\n",
    "    #\n",
    "    # RESULT: All scores now range from 0.0 (worst) to 1.0 (best)\n",
    "    #\n",
    "    # The +1e-10 prevents division by zero if all scores are identical\n",
    "    # ========================================================================\n",
    "    bm25_scores_list = list(bm25_scores_dict.values())\n",
    "    bm25_min = min(bm25_scores_list)\n",
    "    bm25_max = max(bm25_scores_list)\n",
    "    \n",
    "    bm25_normalized_dict = {}\n",
    "    for chunk_idx, score in bm25_scores_dict.items():\n",
    "        bm25_normalized_dict[chunk_idx] = (score - bm25_min) / (bm25_max - bm25_min + 1e-10)\n",
    "    \n",
    "    # STEP 4: Combine both scores with weighted average\n",
    "    # ========================================================================\n",
    "    # Œ± (alpha) controls the balance between semantic and keyword search\n",
    "    # ========================================================================\n",
    "    # Œ± = 0.7 means: hybrid_score = 0.7 √ó vector_score + 0.3 √ó bm25_score\n",
    "    #\n",
    "    # Adjust Œ± based on your use case:\n",
    "    # - Œ± = 0.0 ‚Üí Pure keyword search (BM25 only)\n",
    "    # - Œ± = 0.3 ‚Üí Favor keywords (good for exact term matching)\n",
    "    # - Œ± = 0.5 ‚Üí Equal balance\n",
    "    # - Œ± = 0.7 ‚Üí Favor semantics (good for conceptual queries) ‚Üê USING THIS\n",
    "    # - Œ± = 1.0 ‚Üí Pure semantic search (vector only)\n",
    "    # ========================================================================\n",
    "    alpha = 0.7  # 70% vector, 30% keyword\n",
    "    \n",
    "    # Calculate hybrid scores for the top 3 chunks\n",
    "    hybrid_scores_dict = {}\n",
    "    for chunk_idx in vector_scores_dict.keys():\n",
    "        vector_score = vector_scores_dict[chunk_idx]\n",
    "        bm25_score = bm25_normalized_dict[chunk_idx]\n",
    "        hybrid_scores_dict[chunk_idx] = alpha * vector_score + (1 - alpha) * bm25_score\n",
    "    \n",
    "    # STEP 5: Rank chunks by combined hybrid score\n",
    "    # Sort the chunks by hybrid score (highest first)\n",
    "    sorted_hybrid = sorted(hybrid_scores_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"Top 3 hybrid results (Œ±={alpha}: {int(alpha*100)}% vector, \" +\n",
    "          f\"{int((1-alpha)*100)}% keyword):\\n\")\n",
    "    for rank, (chunk_idx, hybrid_score) in enumerate(sorted_hybrid, 1):\n",
    "        vector_score = vector_scores_dict[chunk_idx]\n",
    "        bm25_score = bm25_normalized_dict[chunk_idx]\n",
    "        print(f\"Rank {rank} | Hybrid: {hybrid_score:.4f} \" +\n",
    "              f\"(Vector: {vector_score:.4f}, Keyword: {bm25_score:.4f})\")\n",
    "        print(f\"Content: {chunks_to_embed[chunk_idx][:150]}...\")\n",
    "        print()\n",
    "    \n",
    "    print(\"üí° Adjust Œ±: <0.5 favors keywords (exact matches), \" +\n",
    "          \">0.5 favors semantics (meaning)\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  BM25 not installed. Run: pip install rank-bm25\")\n",
    "    print(\"   Showing vector search results only.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"KEY OBSERVATIONS:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"‚Ä¢ Vector search uses HNSW index for fast approximate search\")\n",
    "print(\"‚Ä¢ BM25 provides exact keyword matching for precision\")\n",
    "print(\"‚Ä¢ Hybrid search combines BOTH: semantic understanding + keyword relevance\")\n",
    "print(\"‚Ä¢ This is the 'Retrieval' part of RAG (Retrieval-Augmented Generation)\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
