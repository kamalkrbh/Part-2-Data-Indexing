{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e14fb7c",
   "metadata": {},
   "source": [
    "## Setup: Import Required Libraries\n",
    "\n",
    "First, ensure you've installed dependencies from `requirements.txt`:\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Now let's import the required libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94eeea2",
   "metadata": {},
   "source": [
    "## Semantic Chunking Function\n",
    "\n",
    "We'll use semantic chunking to split our text into meaningful chunks based on topic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb560bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sample data and required libraries\n",
    "from sample_data import SAMPLE_TEXT\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def chunk_by_semantic_similarity(text: str, similarity_threshold: float = 0.5, overlap_sentences: int = 2, min_chunk_size: int = 2) -> list:\n",
    "    \"\"\"\n",
    "    Semantic chunking based on sentence similarity using TF-IDF vectors\n",
    "    \n",
    "    Parameters:\n",
    "    - similarity_threshold: Cosine similarity threshold (0-1). Lower = more chunks\n",
    "    - overlap_sentences: Number of sentences from previous chunk to include as context\n",
    "    - min_chunk_size: Minimum number of sentences per chunk\n",
    "    \n",
    "    Groups semantically similar sentences together and creates boundaries where\n",
    "    the semantic topic shifts (similarity drops below threshold).\n",
    "    \"\"\"\n",
    "    # STEP 1: Split the text into individual sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    if len(sentences) <= min_chunk_size:\n",
    "        return [text]\n",
    "    \n",
    "    # STEP 2: Convert sentences to numerical vectors using TF-IDF\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    try:\n",
    "        sentence_vectors = vectorizer.fit_transform(sentences)\n",
    "    except ValueError:\n",
    "        return [text]\n",
    "    \n",
    "    # STEP 3: Calculate similarity between consecutive sentences\n",
    "    similarities = [cosine_similarity(sentence_vectors[i:i+1], sentence_vectors[i+1:i+2])[0][0] \n",
    "                   for i in range(len(sentences) - 1)]\n",
    "    \n",
    "    # STEP 4: Identify chunk boundaries where topics change\n",
    "    chunk_boundaries = [0]\n",
    "    current_chunk_size = 1\n",
    "    \n",
    "    for i, sim in enumerate(similarities):\n",
    "        if sim < similarity_threshold and current_chunk_size >= min_chunk_size:\n",
    "            chunk_boundaries.append(i + 1)\n",
    "            current_chunk_size = 1\n",
    "        else:\n",
    "            current_chunk_size += 1\n",
    "    \n",
    "    if chunk_boundaries[-1] != len(sentences):\n",
    "        chunk_boundaries.append(len(sentences))\n",
    "    \n",
    "    # STEP 5: Create actual text chunks with optional overlap\n",
    "    chunks = []\n",
    "    for i in range(len(chunk_boundaries) - 1):\n",
    "        start_idx = chunk_boundaries[i]\n",
    "        end_idx = chunk_boundaries[i + 1]\n",
    "        \n",
    "        if i > 0 and overlap_sentences > 0:\n",
    "            overlap_start = max(0, start_idx - overlap_sentences)\n",
    "            chunk_sentences = sentences[overlap_start:end_idx]\n",
    "        else:\n",
    "            chunk_sentences = sentences[start_idx:end_idx]\n",
    "        \n",
    "        chunk = \" \".join(chunk_sentences)\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Create semantic chunks\n",
    "chunks = chunk_by_semantic_similarity(SAMPLE_TEXT, similarity_threshold=0.15, overlap_sentences=2)\n",
    "print(f\"‚úÖ Created {len(chunks)} semantic chunks\\n\")\n",
    "\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"Chunk {i} ({len(chunk)} chars): {chunk[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc24dc4",
   "metadata": {},
   "source": [
    "## Build Vector Store\n",
    "\n",
    "Now we'll create embeddings for our chunks and store them in ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30d936a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize embedding model\n",
    "print(\"üîÑ Loading embedding model...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')  # 384-dimensional embeddings\n",
    "print(\"‚úÖ Embedding model loaded!\\n\")\n",
    "\n",
    "# Generate embeddings for all chunks\n",
    "print(f\"üîÑ Generating embeddings for {len(chunks)} chunks...\")\n",
    "embeddings = embedding_model.encode(chunks, show_progress_bar=True)\n",
    "print(f\"‚úÖ Generated {len(embeddings)} embeddings, each with {embeddings[0].shape[0]} dimensions\\n\")\n",
    "\n",
    "# Initialize ChromaDB\n",
    "print(\"üîÑ Initializing ChromaDB vector store...\")\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# Create collection with cosine similarity\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"retrieval_demo_chunks\",\n",
    "    metadata={\n",
    "        \"hnsw:space\": \"cosine\",\n",
    "        \"description\": \"Chunks for retrieval strategy demonstration\"\n",
    "    }\n",
    ")\n",
    "print(\"‚úÖ ChromaDB collection created!\\n\")\n",
    "\n",
    "# Store chunks with embeddings\n",
    "print(f\"üîÑ Storing {len(chunks)} chunks in vector database...\")\n",
    "collection.add(\n",
    "    ids=[f\"chunk_{i}\" for i in range(len(chunks))],\n",
    "    embeddings=embeddings.tolist(),\n",
    "    documents=chunks,\n",
    "    metadatas=[{\"chunk_index\": i, \"length\": len(chunk)} for i, chunk in enumerate(chunks)]\n",
    ")\n",
    "print(\"‚úÖ All chunks stored in vector database!\\n\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"VECTOR STORE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"‚Ä¢ Total chunks stored: {collection.count()}\")\n",
    "print(f\"‚Ä¢ Embedding dimensions: {embeddings[0].shape[0]}\")\n",
    "print(f\"‚Ä¢ Similarity metric: Cosine\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50998b7",
   "metadata": {},
   "source": [
    "## 1. Retrieval Strategies\n",
    "\n",
    "Different strategies for fetching relevant chunks:\n",
    "- **Dense (Vector Search)**: Uses embeddings for semantic similarity\n",
    "- **Hybrid (BM25 + Embeddings)**: Combines keyword matching with semantic search (recommended for production)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14c098e",
   "metadata": {},
   "source": [
    "### 1.1 Dense Retrieval (Vector Search Only)\n",
    "\n",
    "Pure semantic similarity using embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f818a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_retrieval(query: str, top_k: int = 3):\n",
    "    \"\"\"\n",
    "    Dense retrieval using vector embeddings only.\n",
    "    Returns top_k most semantically similar chunks.\n",
    "    \"\"\"\n",
    "    # Convert query to embedding\n",
    "    query_embedding = embedding_model.encode([query])[0]\n",
    "    \n",
    "    # Search vector database\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding.tolist()],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test dense retrieval\n",
    "query = \"What are the internet speed requirements for remote work?\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DENSE RETRIEVAL (Vector Search Only)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüîç Query: '{query}'\\n\")\n",
    "\n",
    "results = dense_retrieval(query, top_k=3)\n",
    "\n",
    "print(\"Top 3 results by semantic similarity:\\n\")\n",
    "for i, (doc, metadata, distance) in enumerate(zip(\n",
    "    results['documents'][0], \n",
    "    results['metadatas'][0],\n",
    "    results['distances'][0]\n",
    "), 1):\n",
    "    similarity_score = 1 - distance\n",
    "    print(f\"Rank {i} | Similarity: {similarity_score:.4f}\")\n",
    "    print(f\"Content: {doc}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üí° Dense retrieval captures semantic meaning but may miss exact keywords\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d2188a",
   "metadata": {},
   "source": [
    "### 1.2 Hybrid Retrieval (BM25 + Embeddings)\n",
    "\n",
    "Combines keyword matching (BM25) with semantic search for best results.\n",
    "\n",
    "**What is BM25?**\n",
    "\n",
    "BM25 (Best Matching 25) is a ranking function used in information retrieval that scores documents based on the query terms appearing in each document. \n",
    "\n",
    "**Why \"25\"?** The \"25\" simply indicates it's the 25th iteration/variation in the \"Best Matching\" family of ranking functions developed by researchers. Earlier versions included BM1, BM11, BM15, etc. BM25 emerged as the most effective version and became the standard.\n",
    "\n",
    "**How BM25 works - Key components:**\n",
    "\n",
    "1. **Term Frequency (TF)**: How often a query term appears in a document\n",
    "   - **Query term**: A word from your search query (e.g., if you search \"internet speed\", the query terms are \"internet\" and \"speed\")\n",
    "   - **Diminishing returns**: The relevance benefit of each additional occurrence decreases. Here's why:\n",
    "     - **Real-world intuition**: If you're searching for \"python tutorial\" and find a document:\n",
    "       - With 1 occurrence: \"This Python tutorial covers basics\" ‚Üí Clearly relevant ‚úì\n",
    "       - With 3 occurrences: Document discusses Python tutorials multiple times ‚Üí More relevant ‚úì‚úì\n",
    "       - With 100 occurrences: Likely keyword stuffing or spam (\"python python python tutorial tutorial...\") ‚Üí Not 100x more useful!\n",
    "     - **The problem with linear counting**: If we simply count occurrences, spam documents that repeat words would always rank #1\n",
    "     - **BM25's solution**: Uses a saturation function that increases score rapidly at first, then flattens out\n",
    "     - **Mathematical behavior**: \n",
    "       - 1 occurrence ‚Üí score: 1.0\n",
    "       - 2 occurrences ‚Üí score: 1.5 (50% increase)\n",
    "       - 3 occurrences ‚Üí score: 1.8 (20% increase)\n",
    "       - 5 occurrences ‚Üí score: 2.1 (smaller increase)\n",
    "       - 100 occurrences ‚Üí score: 2.5 (barely any increase from 5 to 100!)\n",
    "     - **Key insight**: After ~5-10 occurrences, additional mentions add very little value - the curve \"saturates\"\n",
    "   \n",
    "2. **Inverse Document Frequency (IDF)**: How rare/common a term is across all documents\n",
    "   - **Inverse**: \"Opposite\" or \"reverse\" - we want RARE terms to score HIGHER\n",
    "   - **Document Frequency**: How many documents contain this term\n",
    "   - **Logic**: If \"the\" appears in 1000/1000 documents, it tells us nothing useful (low IDF weight)\n",
    "   - **Logic**: If \"photosynthesis\" appears in 2/1000 documents, it's highly specific (high IDF weight)\n",
    "   - **Formula**: IDF = log(total_documents / documents_containing_term)\n",
    "   - **Example**: \n",
    "     - \"the\" appears in 950/1000 docs ‚Üí IDF = log(1000/950) = 0.05 (low weight)\n",
    "     - \"photosynthesis\" appears in 2/1000 docs ‚Üí IDF = log(1000/2) = 2.7 (high weight)\n",
    "   \n",
    "3. **Document Length Normalization**: Adjusts scores based on document length\n",
    "   - **Problem**: Longer documents naturally contain more words, giving them unfair advantage\n",
    "   - **Solution**: Normalize scores by document length relative to average document length\n",
    "   - **Example**: A 1000-word document with 3 matches isn't necessarily better than a 100-word document with 2 matches\n",
    "\n",
    "**Why Hybrid (BM25 + Embeddings)?**\n",
    "\n",
    "- **BM25 strengths**: Excellent at finding exact keyword/phrase matches (e.g., \"Article 5\", \"GDP growth\")\n",
    "- **Embedding strengths**: Understands semantic meaning and synonyms (e.g., \"WiFi\" ‚âà \"internet\", \"quick\" ‚âà \"fast\")\n",
    "- **Combined**: Handles both \"WiFi requirements\" and \"internet speed needs\" queries effectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9c4623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "\n",
    "def hybrid_retrieval(query: str, top_k: int = 3, alpha: float = 0.7):\n",
    "    \"\"\"\n",
    "    Hybrid retrieval combining BM25 keyword matching with vector embeddings.\n",
    "    \n",
    "    This function implements a two-stage retrieval approach:\n",
    "    1. Use vector embeddings to capture semantic similarity\n",
    "    2. Use BM25 to capture exact keyword/term matches\n",
    "    3. Combine both scores for robust retrieval\n",
    "    \n",
    "    Parameters:\n",
    "    - query: Search query string from the user\n",
    "    - top_k: Number of results to return (default: 3)\n",
    "    - alpha: Weight for vector score (0-1). Higher = more semantic, lower = more keyword\n",
    "             Example: alpha=0.7 means 70% vector + 30% BM25\n",
    "    \n",
    "    Returns: List of dictionaries containing chunk indices, scores, and content\n",
    "    \"\"\"\n",
    "    # Step 1: Dense retrieval (get top candidates by vector similarity)\n",
    "    # Convert the query text into a numerical embedding vector\n",
    "    query_embedding = embedding_model.encode([query])[0]\n",
    "    \n",
    "    # Query the vector database to find semantically similar chunks\n",
    "    vector_results = collection.query(\n",
    "        query_embeddings=[query_embedding.tolist()],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    \n",
    "    # Extract chunk indices and vector scores from results\n",
    "    # Vector scores are calculated as (1 - cosine_distance) for similarity\n",
    "    vector_scores_dict = {}\n",
    "    for chunk_id, distance in zip(vector_results['ids'][0], vector_results['distances'][0]):\n",
    "        chunk_idx = int(chunk_id.split('_')[1])  # Extract numeric index from 'chunk_0', 'chunk_1', etc.\n",
    "        vector_scores_dict[chunk_idx] = 1 - distance  # Convert distance to similarity\n",
    "    \n",
    "    # Step 2: BM25 keyword matching\n",
    "    # Tokenize all chunks into words for BM25 processing\n",
    "    tokenized_chunks = [chunk.lower().split() for chunk in chunks]\n",
    "    \n",
    "    query_tokens = query.lower().split()\n",
    "    # Initialize BM25 algorithm with tokenized chunks\n",
    "    # BM25 uses term frequency and inverse document frequency for scoring\n",
    "    bm25 = BM25Okapi(tokenized_chunks)\n",
    "    \n",
    "    # Get BM25 scores for the top_k chunks identified by vector search\n",
    "    # This ensures we only score chunks that are already semantically relevant\n",
    "    bm25_scores_dict = {}\n",
    "    for chunk_idx in vector_scores_dict.keys():\n",
    "        # bm25.get_scores() returns scores for ALL chunks in the corpus\n",
    "        # We index it with [chunk_idx] to get the score for this specific chunk\n",
    "        # Example: if chunk_idx=2, we get the BM25 score for the 3rd chunk (0-indexed)\n",
    "        bm25_score = bm25.get_scores(query_tokens)[chunk_idx]\n",
    "        \n",
    "        # Store the BM25 score for this chunk in our dictionary\n",
    "        # This allows us to later combine it with the vector score\n",
    "        bm25_scores_dict[chunk_idx] = bm25_score\n",
    "    \n",
    "    # Step 3: Normalize BM25 scores to 0-1 range\n",
    "    # This makes BM25 scores comparable with vector scores (which are already 0-1)\n",
    "    bm25_scores_list = list(bm25_scores_dict.values())\n",
    "    bm25_min = min(bm25_scores_list)\n",
    "    bm25_max = max(bm25_scores_list)\n",
    "    # Min-max normalization: (score - min) / (max - min)\n",
    "    # Add small epsilon (1e-10) to avoid division by zero\n",
    "    bm25_scores_normalized_dict = {}\n",
    "    for chunk_idx, score in bm25_scores_dict.items():\n",
    "        bm25_scores_normalized_dict[chunk_idx] = (score - bm25_min) / (bm25_max - bm25_min + 1e-10)\n",
    "    \n",
    "    # Step 4: Combine scores with weighted average\n",
    "    # Final score = (alpha √ó vector_score) + ((1-alpha) √ó bm25_score)\n",
    "    # This balances semantic understanding with keyword matching\n",
    "    hybrid_scores_dict = {}\n",
    "    for chunk_idx in vector_scores_dict.keys():\n",
    "        vector_score = vector_scores_dict[chunk_idx]\n",
    "        bm25_score = bm25_scores_normalized_dict[chunk_idx]\n",
    "        hybrid_scores_dict[chunk_idx] = alpha * vector_score + (1 - alpha) * bm25_score\n",
    "    \n",
    "    # Step 5: Sort by hybrid score (highest first)\n",
    "    # This gives us the most relevant chunks based on combined scoring\n",
    "    sorted_results = sorted(hybrid_scores_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Build final results list with all relevant information\n",
    "    # Include individual scores for transparency and debugging\n",
    "    results = []\n",
    "    for chunk_idx, hybrid_score in sorted_results:\n",
    "        results.append({\n",
    "            'chunk_idx': chunk_idx,\n",
    "            'hybrid_score': hybrid_score,\n",
    "            'vector_score': vector_scores_dict[chunk_idx],\n",
    "            'bm25_score': bm25_scores_normalized_dict[chunk_idx],\n",
    "            'content': chunks[chunk_idx]\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test hybrid retrieval with a sample query\n",
    "print(\"=\" * 80)\n",
    "print(\"HYBRID RETRIEVAL (BM25 + Embeddings)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüîç Query: '{query}'\\n\")\n",
    "\n",
    "# Run hybrid retrieval with alpha=0.7 (70% semantic, 30% keyword)\n",
    "hybrid_results = hybrid_retrieval(query, top_k=3, alpha=0.7)\n",
    "\n",
    "# Display results with scores breakdown\n",
    "print(f\"Top 3 hybrid results (Œ±=0.7: 70% semantic, 30% keyword):\\n\")\n",
    "for i, result in enumerate(hybrid_results, 1):\n",
    "    print(f\"Rank {i} | Hybrid: {result['hybrid_score']:.4f} \" +\n",
    "          f\"(Vector: {result['vector_score']:.4f}, BM25: {result['bm25_score']:.4f})\")\n",
    "    print(f\"Content: {result['content'][:150]}...\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üí° Hybrid retrieval combines semantic understanding with keyword matching\")\n",
    "print(\"   Adjust Œ±: <0.5 favors keywords, >0.5 favors semantics\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeee18b",
   "metadata": {},
   "source": [
    "### 2. Query Expansion: Improving Retrieval with Better Queries\n",
    "\n",
    "**Problem:** Users often ask queries in ways that don't match document terminology.\n",
    "\n",
    "**Examples:**\n",
    "- User: \"How fast should my internet be?\" ‚Üí Document: \"minimum bandwidth requirements\"\n",
    "- User: \"WFH guidelines\" ‚Üí Document: \"remote work policy\"\n",
    "- User: \"What's needed for home office?\" ‚Üí Document: \"equipment specifications\"\n",
    "\n",
    "**Solution:** Use an LLM to expand or rewrite queries for better matching.\n",
    "\n",
    "#### LLM-Based Query Expansion\n",
    "\n",
    "1. **Intelligent Synonyms**: LLMs understand context and generate appropriate synonyms\n",
    "   - \"fast internet\" ‚Üí \"high-speed connection, broadband, bandwidth requirements\"\n",
    "\n",
    "2. **Acronym Handling**: Automatically expands abbreviations\n",
    "   - \"WFH\" ‚Üí \"work from home, remote work, telecommuting\"\n",
    "\n",
    "3. **Contextual Rewriting**: Rephrases queries to match document style\n",
    "   - \"How fast should internet be?\" ‚Üí \"internet speed requirements, minimum bandwidth specifications\"\n",
    "\n",
    "4. **Multi-Query Generation**: Creates diverse query variations\n",
    "   - Original: \"remote work requirements\"\n",
    "   - Variant 1: \"work from home eligibility criteria\"\n",
    "   - Variant 2: \"home office setup guidelines\"\n",
    "   - Retrieve from all, deduplicate results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72137a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "groq_client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "print(\"‚úÖ LLM client initialized successfully!\\n\")\n",
    "\n",
    "def llm_query_expansion(query: str, num_variations: int = 3):\n",
    "    prompt = f\"\"\"You are a search query optimizer. Given a user's search query, generate {num_variations} \n",
    "    improved variations that will help find relevant documents.\\n\\nEach variation should:\\n- \n",
    "    Use different words/synonyms but preserve the same search intent\\n-\n",
    "    Expand abbreviations (e.g., 'WFH' ‚Üí 'work from home')\\n- \n",
    "    Add relevant terms that might appear in documents\\n- \n",
    "    Use both casual and formal terminology where appropriate\\n\\nOriginal query: {query}\\n\\n\n",
    "    Return ONLY {num_variations} optimized query variations, one per line, numbered 1-{num_variations}:\"\"\"\n",
    "    response = groq_client.chat.completions.create(\n",
    "        model=\"llama-3.1-8b-instant\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.7,\n",
    "        max_tokens=150\n",
    "    )\n",
    "    variations_text = response.choices[0].message.content.strip()\n",
    "    variations = [query]\n",
    "    for line in variations_text.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if line and (line[0].isdigit() or line.startswith('-') or line.startswith('‚Ä¢')):\n",
    "            cleaned = line.lstrip('0123456789.-‚Ä¢) ').strip()\n",
    "            if cleaned and cleaned not in variations:\n",
    "                variations.append(cleaned)\n",
    "    return variations[:num_variations + 1]\n",
    "\n",
    "def multi_query_retrieval(query: str, top_k: int = 3, num_variations: int = 2):\n",
    "    query_variations = llm_query_expansion(query, num_variations=num_variations)\n",
    "    all_results = {}\n",
    "    for variation in query_variations:\n",
    "        results = hybrid_retrieval(variation, top_k=top_k * 2, alpha=0.7)\n",
    "        for result in results:\n",
    "            chunk_idx = result['chunk_idx']\n",
    "            if chunk_idx not in all_results or result['hybrid_score'] > all_results[chunk_idx]['hybrid_score']:\n",
    "                all_results[chunk_idx] = result\n",
    "    sorted_results = sorted(all_results.values(), key=lambda x: x['hybrid_score'], reverse=True)[:top_k]\n",
    "    return sorted_results\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LLM-BASED QUERY EXPANSION & MULTI-QUERY RETRIEVAL DEMO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_query = \"What are the WFH internet requirements?\"\n",
    "print(f\"User Query: {test_query}\\n\")\n",
    "query_variations = llm_query_expansion(test_query, num_variations=3)\n",
    "\n",
    "# DEMO 1: Retrieval using the ORIGINAL query\n",
    "original_results = hybrid_retrieval(test_query, top_k=3, alpha=0.7)\n",
    "print(\"-\" * 80)\n",
    "print(\"DEMO 1: Retrieval using the ORIGINAL query\")\n",
    "print(\"-\" * 80)\n",
    "for i, res in enumerate(original_results, 1):\n",
    "    print(f\"Result {i} | Score: {res['hybrid_score']:.4f}\\n  Content: {res['content'][:200]}...\\n\")\n",
    "\n",
    "# DEMO 2: Retrieval using the FIRST expanded query\n",
    "print(\"-\" * 80)\n",
    "print(\"DEMO 2: Retrieval using the FIRST expanded query\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Using expanded query: '{query_variations[1]}'\")\n",
    "expanded_results = hybrid_retrieval(query_variations[1], top_k=3, alpha=0.7)\n",
    "for i, res in enumerate(expanded_results, 1):\n",
    "    print(f\"Result {i} | Score: {res['hybrid_score']:.4f}\\n  Content: {res['content'][:200]}...\\n\")\n",
    "\n",
    "# DEMO 3: Multi-query retrieval (aggregate best matches from ALL expanded queries)\n",
    "print(\"-\" * 80)\n",
    "print(\"DEMO 3: MULTI-QUERY RETRIEVAL (aggregate best matches from ALL expanded queries)\")\n",
    "print(\"-\" * 80)\n",
    "test_query2 = \"What are the internet speed requirements for remote work?\"\n",
    "multi_results = multi_query_retrieval(test_query2, top_k=3, num_variations=2)\n",
    "for i, res in enumerate(multi_results, 1):\n",
    "    print(f\"Result {i} | Score: {res['hybrid_score']:.4f}\\n  Content: {res['content'][:200]}...\\n\")\n",
    "\n",
    "# Crisp Score Comparison Table\n",
    "print(\"-\" * 80)\n",
    "print(\"SCORE COMPARISON TABLE (Scores Only)\")\n",
    "print(f\"{'Demo':<30}{'R1':>8}{'R2':>8}{'R3':>8}\")\n",
    "print(f\"{'-'*30}{'-'*8}{'-'*8}{'-'*8}\")\n",
    "print(f\"{'Original Query':<30}{original_results[0]['hybrid_score']:>8.4f}{original_results[1]['hybrid_score']:>8.4f}{original_results[2]['hybrid_score']:>8.4f}\")\n",
    "print(f\"{'First Expanded Query':<30}{expanded_results[0]['hybrid_score']:>8.4f}{expanded_results[1]['hybrid_score']:>8.4f}{expanded_results[2]['hybrid_score']:>8.4f}\")\n",
    "print(f\"{'Multi-Query Retrieval':<30}{multi_results[0]['hybrid_score']:>8.4f}{multi_results[1]['hybrid_score']:>8.4f}{multi_results[2]['hybrid_score']:>8.4f}\")\n",
    "print(f\"{'-'*30}{'-'*8}{'-'*8}{'-'*8}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdf6067",
   "metadata": {},
   "source": [
    "## 3. Re-ranking: Post-filtering for Better Relevance\n",
    "\n",
    "Re-ranking refines initial retrieval results using more sophisticated models.\n",
    "\n",
    "**Common approaches:**\n",
    "- **Cross-encoder rerankers**: More accurate but slower (e.g., `ms-marco-MiniLM`)\n",
    "- **LLM-based reranking**: Use LLM to score relevance\n",
    "- **Metadata filtering**: Filter by user role, time window, etc.\n",
    "\n",
    "**Strategy:**\n",
    "1. Retrieve larger set (e.g., TOP-K=10) with fast retrieval\n",
    "2. Rerank to identify best 3-5 chunks\n",
    "3. Use reranked results for generation\n",
    "\n",
    "**How is the cross-encoder score different from dense vector search and BM25 scores?**\n",
    "\n",
    "- **Dense vector search score**: This uses embeddings to measure how similar the query and chunk are in meaning. Each is turned into a vector separately, and their similarity (like cosine similarity) is calculated. It captures overall semantic similarity but doesn't look at the exact way the query and chunk relate word-by-word.\n",
    "\n",
    "- **BM25 score**: This is a traditional keyword-based method. It scores based on how many query words appear in the chunk, how rare those words are, and document length. It‚Äôs great for exact matches but doesn‚Äôt understand meaning or synonyms.\n",
    "\n",
    "- **Cross-encoder score**: Here, a neural network looks at the query and chunk together, considering the full context of both at once. It can understand complex relationships, word order, and deeper meaning, making it more accurate for ranking relevance. It‚Äôs slower, but often gives better results for the top matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdecfce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Encoder Reranker using sentence-transformers\n",
    "from sentence_transformers import CrossEncoder\n",
    "import torch\n",
    "\n",
    "# --- CROSS-ENCODER RERANKING ---\n",
    "# This approach uses a neural network to jointly encode the query and each candidate chunk,\n",
    "# then predicts a relevance score for each pair. This is much more accurate than simple scoring,\n",
    "# but also slower, so it's best used on a small set of top candidates (e.g., top 5-10 from initial retrieval).\n",
    "\n",
    "# Load a cross-encoder model (downloads weights if not cached; may take a while the first time)\n",
    "# The neural network is inside the CrossEncoder model. \n",
    "# When you create cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2'), \n",
    "# you are loading a pre-trained neural network that has learned to judge how well a query matches a chunk of text. \n",
    "if 'cross_encoder' not in globals():\n",
    "    print(\"\\nüîÑ Loading cross-encoder model for reranking...\")\n",
    "    cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "    print(\"‚úÖ Cross-encoder loaded!\\n\")\n",
    "\n",
    "def cross_encoder_reranker(query, initial_results, top_n=3):\n",
    "    \"\"\"\n",
    "    Rerank initial results using a cross-encoder model for more accurate relevance scoring.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user query.\n",
    "        initial_results (list): List of dicts with 'content' key for each chunk (from initial retrieval).\n",
    "        top_n (int): Number of top results to return after reranking.\n",
    "    \n",
    "    Returns:\n",
    "        List of dicts, reranked by cross-encoder score (descending).\n",
    "    \"\"\"\n",
    "    # Prepare input pairs: (query, chunk) for each candidate chunk.\n",
    "    # The cross-encoder will score each pair for relevance.\n",
    "    pairs = [(query, result['content']) for result in initial_results]\n",
    "    \n",
    "    # Get relevance scores from the cross-encoder model.\n",
    "    # The model outputs a score for each (query, chunk) pair: higher = more relevant.\n",
    "    # We use torch.no_grad() to avoid tracking gradients (not needed for inference).\n",
    "    \n",
    "    # This line tells PyTorch not to track gradients while running the code inside it. \n",
    "    # A gradient is just the slope or rate of change of something‚Äîin machine learning, \n",
    "    # gradients show how much the model's output changes as you change its parameters. \n",
    "    # We use torch.no_grad() during prediction (inference), not training, to save memory and make things faster, \n",
    "    # since gradients are only needed for learning, not for making predictions.\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # This line asks the cross-encoder model to look at each pair of (query, chunk) \n",
    "        # and give a number showing how well the chunk matches the query. \n",
    "        # The higher the number, the more relevant the chunk is to what the user asked. \n",
    "        # These scores are then used to sort and pick the best answers.\n",
    "        scores = cross_encoder.predict(pairs)\n",
    "    \n",
    "    # Attach the cross-encoder score to each result and build a new list.\n",
    "    reranked = []\n",
    "    for result, score in zip(initial_results, scores):\n",
    "        result_ce = result.copy()  # Copy original result dict\n",
    "        result_ce['cross_encoder_score'] = float(score)  # Add the new score\n",
    "        reranked.append(result_ce)\n",
    "    \n",
    "    # Sort all results by cross-encoder score (highest first)\n",
    "    reranked.sort(key=lambda x: x['cross_encoder_score'], reverse=True)\n",
    "    return reranked[:top_n]  # Return only the top_n results\n",
    "\n",
    "# --- DEMONSTRATION: Cross-Encoder Reranking ---\n",
    "if '_ce_demo_ran' not in globals():\n",
    "    query = \"What are the internet speed requirements for remote work?\"\n",
    "\n",
    "    # Step 1: Hybrid search results and scores\n",
    "    print(\"=\" * 80)\n",
    "    print(\"STEP 1 - Hybrid search results and scores (Top 5)\")\n",
    "    print(\"=\" * 80)\n",
    "    initial_results = hybrid_retrieval(query, top_k=5, alpha=0.7)\n",
    "    for i, result in enumerate(initial_results, 1):\n",
    "        print(f\"Rank {i} | Hybrid Score: {result['hybrid_score']:.4f} | Vector: {result['vector_score']:.4f} | BM25: {result['bm25_score']:.4f}\")\n",
    "        print(f\"  Content: {result['content'][:100]}...\\n\")\n",
    "\n",
    "    # Step 2: Cross-encoder results and scores\n",
    "    print(\"-\" * 80)\n",
    "    print(\"STEP 2 - Cross-encoder reranking results and scores (Top 3)\")\n",
    "    print(\"-\" * 80)\n",
    "    reranked_results = cross_encoder_reranker(query, initial_results, top_n=3)\n",
    "    for i, result in enumerate(reranked_results, 1):\n",
    "        print(f\"Rank {i} | Cross-Encoder Score: {result['cross_encoder_score']:.4f}\")\n",
    "        print(f\"  Hybrid Score: {result['hybrid_score']:.4f} | Vector: {result['vector_score']:.4f} | BM25: {result['bm25_score']:.4f}\")\n",
    "        print(f\"  Content: {result['content'][:100]}...\\n\")\n",
    "\n",
    "    # Step 3: Comparison table sorted by cross-encoder score\n",
    "    print(\"=\" * 80)\n",
    "    print(\"FINAL COMPARISON: Top 5 Hybrid Results Sorted by Cross-Encoder Score\")\n",
    "    print(\"=\" * 80)\n",
    "    # Attach cross-encoder scores to all initial results for fair comparison\n",
    "    all_pairs = [(query, result['content']) for result in initial_results]\n",
    "    with torch.no_grad():\n",
    "        all_scores = cross_encoder.predict(all_pairs)\n",
    "    for i, (result, ce_score) in enumerate(sorted(zip(initial_results, all_scores), key=lambda x: x[1], reverse=True), 1):\n",
    "        print(f\"Rank {i} | Cross-Encoder Score: {ce_score:.4f} | Hybrid Score: {result['hybrid_score']:.4f}\")\n",
    "        print(f\"  Content: {result['content'][:100]}...\\n\")\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"üí° Cross-encoder reranking provides a more accurate final ranking by deeply analyzing the query and chunk together.\")\n",
    "    print(\"   Use hybrid search for recall, then cross-encoder for precision in the final answer.\")\n",
    "    print(\"=\" * 80)\n",
    "    _ce_demo_ran = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b7e482",
   "metadata": {},
   "source": [
    "## 4. Context Filtering: Deciding Relevance to Query Intent\n",
    "\n",
    "Context filtering ensures that retrieved chunks are actually relevant to the user's intent and the needs of the downstream application. This step is crucial for improving the quality of answers and reducing irrelevant or distracting information.\n",
    "\n",
    "### Key Context Filtering Techniques (Theory)\n",
    "\n",
    "- **Similarity Threshold Filtering:**\n",
    "  - Only include chunks whose similarity or relevance score (from hybrid or cross-encoder reranker) is above a certain threshold.\n",
    "  - Lower thresholds include more context (risking noise), higher thresholds are stricter (risking missing useful info).\n",
    "\n",
    "- **Top-N Filtering:**\n",
    "  - Return only the top N most relevant chunks, regardless of their absolute score.\n",
    "  - Useful for limiting context size for LLMs or downstream models.\n",
    "\n",
    "- **Metadata Filtering:**\n",
    "  - Use metadata fields (e.g., document type, user role, department, date) to include or exclude chunks.\n",
    "  - Example: Only show policy documents to HR users, or filter by recent date for time-sensitive queries.\n",
    "\n",
    "- **Intent Detection and Filtering:**\n",
    "  - Classify the user's query to understand what kind of information they are seeking (e.g., 'policy', 'requirements', 'troubleshooting').\n",
    "  - Filter or boost chunks that match the detected intent, either by keyword, metadata, or LLM-based classification.\n",
    "  - Suppose a user query is:\n",
    "  Example:\n",
    "      > \"What are the troubleshooting steps for slow internet?\"\n",
    "\n",
    "    **Step 1: Detect Intent**\n",
    "    - Use simple keyword matching or rules:\n",
    "      - If the query contains words like \"troubleshooting\", \"fix\", \"resolve\", classify intent as `troubleshooting`.\n",
    "      - If the query contains \"requirements\", intent is `requirements`.\n",
    "      - If the query contains \"policy\", intent is `policy`.\n",
    "    \n",
    "    After retrieval, only include or boost chunks that mention the intent keyword (e.g., \"troubleshooting\") or are tagged as such in metadata.\n",
    "    Such filtering can be done using LLMs.\n",
    "\n",
    "- **Timeline Filtering:**\n",
    "  - Filter or prioritize chunks based on recency or time relevance (e.g., only include documents from the last year).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
