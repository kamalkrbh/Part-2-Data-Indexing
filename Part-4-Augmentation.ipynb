{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41a17386",
   "metadata": {},
   "source": [
    "## Setup: Prerequisites from Previous Parts\n",
    "\n",
    "We'll reuse some components from Part 2 (Data Indexing) and Part 3 (Retrieval Strategies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fa29413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamal/data_indexing/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import sample data and required libraries\n",
    "from sample_data import SAMPLE_TEXT\n",
    "import nltk\n",
    "import os\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "from groq import Groq\n",
    "import torch\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5deecca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created 10 semantic chunks\n"
     ]
    }
   ],
   "source": [
    "# Semantic chunking function (from Part 2)\n",
    "def chunk_by_semantic_similarity(text: str, similarity_threshold: float = 0.5, overlap_sentences: int = 2, min_chunk_size: int = 2) -> list:\n",
    "    \"\"\"Semantic chunking based on sentence similarity using TF-IDF vectors\"\"\"\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    if len(sentences) <= min_chunk_size:\n",
    "        return [text]\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    try:\n",
    "        sentence_vectors = vectorizer.fit_transform(sentences)\n",
    "    except ValueError:\n",
    "        return [text]\n",
    "    \n",
    "    similarities = [cosine_similarity(sentence_vectors[i:i+1], sentence_vectors[i+1:i+2])[0][0] \n",
    "                   for i in range(len(sentences) - 1)]\n",
    "    \n",
    "    chunk_boundaries = [0]\n",
    "    current_chunk_size = 1\n",
    "    \n",
    "    for i, sim in enumerate(similarities):\n",
    "        if sim < similarity_threshold and current_chunk_size >= min_chunk_size:\n",
    "            chunk_boundaries.append(i + 1)\n",
    "            current_chunk_size = 1\n",
    "        else:\n",
    "            current_chunk_size += 1\n",
    "    \n",
    "    if chunk_boundaries[-1] != len(sentences):\n",
    "        chunk_boundaries.append(len(sentences))\n",
    "    \n",
    "    chunks = []\n",
    "    for i in range(len(chunk_boundaries) - 1):\n",
    "        start_idx = chunk_boundaries[i]\n",
    "        end_idx = chunk_boundaries[i + 1]\n",
    "        \n",
    "        if i > 0 and overlap_sentences > 0:\n",
    "            overlap_start = max(0, start_idx - overlap_sentences)\n",
    "            chunk_sentences = sentences[overlap_start:end_idx]\n",
    "        else:\n",
    "            chunk_sentences = sentences[start_idx:end_idx]\n",
    "        \n",
    "        chunk = \" \".join(chunk_sentences)\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Create semantic chunks\n",
    "chunks = chunk_by_semantic_similarity(SAMPLE_TEXT, similarity_threshold=0.15, overlap_sentences=2)\n",
    "print(f\"âœ… Created {len(chunks)} semantic chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64b5b8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Loading embedding model...\n",
      "âœ… Embedding model loaded!\n",
      "\n",
      "ðŸ”„ Generating embeddings for 10 chunks...\n",
      "âœ… Generated 10 embeddings\n",
      "\n",
      "âœ… Vector store ready with 10 chunks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build vector store (from Part 2)\n",
    "print(\"ðŸ”„ Loading embedding model...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"âœ… Embedding model loaded!\\n\")\n",
    "\n",
    "print(f\"ðŸ”„ Generating embeddings for {len(chunks)} chunks...\")\n",
    "embeddings = embedding_model.encode(chunks, show_progress_bar=False)\n",
    "print(f\"âœ… Generated {len(embeddings)} embeddings\\n\")\n",
    "\n",
    "# Initialize ChromaDB\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"augmentation_demo_chunks\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "\n",
    "# Store chunks with metadata\n",
    "collection.add(\n",
    "    ids=[f\"chunk_{i}\" for i in range(len(chunks))],\n",
    "    embeddings=embeddings.tolist(),\n",
    "    documents=chunks,\n",
    "    metadatas=[{\n",
    "        \"chunk_index\": i, \n",
    "        \"length\": len(chunk),\n",
    "        \"source\": \"Remote Work Policy\",\n",
    "        \"topic\": chunk.split('\\n')[0] if '\\n' in chunk else \"General\"\n",
    "    } for i, chunk in enumerate(chunks)]\n",
    ")\n",
    "print(f\"âœ… Vector store ready with {collection.count()} chunks\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee6ae91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Hybrid retrieval function ready!\n"
     ]
    }
   ],
   "source": [
    "# Hybrid retrieval function (from Part 3)\n",
    "def hybrid_retrieval(query: str, top_k: int = 3, alpha: float = 0.7):\n",
    "    \"\"\"Hybrid retrieval combining BM25 + vector embeddings\"\"\"\n",
    "    # Dense retrieval\n",
    "    query_embedding = embedding_model.encode([query])[0]\n",
    "    vector_results = collection.query(\n",
    "        query_embeddings=[query_embedding.tolist()],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    \n",
    "    vector_scores_dict = {}\n",
    "    for chunk_id, distance in zip(vector_results['ids'][0], vector_results['distances'][0]):\n",
    "        chunk_idx = int(chunk_id.split('_')[1])\n",
    "        vector_scores_dict[chunk_idx] = 1 - distance\n",
    "    \n",
    "    # BM25\n",
    "    tokenized_chunks = [chunk.lower().split() for chunk in chunks]\n",
    "    query_tokens = query.lower().split()\n",
    "    bm25 = BM25Okapi(tokenized_chunks)\n",
    "    \n",
    "    bm25_scores_dict = {}\n",
    "    for chunk_idx in vector_scores_dict.keys():\n",
    "        bm25_scores_dict[chunk_idx] = bm25.get_scores(query_tokens)[chunk_idx]\n",
    "    \n",
    "    # Normalize BM25\n",
    "    bm25_scores_list = list(bm25_scores_dict.values())\n",
    "    bm25_min = min(bm25_scores_list)\n",
    "    bm25_max = max(bm25_scores_list)\n",
    "    bm25_scores_normalized_dict = {\n",
    "        chunk_idx: (score - bm25_min) / (bm25_max - bm25_min + 1e-10)\n",
    "        for chunk_idx, score in bm25_scores_dict.items()\n",
    "    }\n",
    "    \n",
    "    # Combine scores\n",
    "    hybrid_scores_dict = {\n",
    "        chunk_idx: alpha * vector_scores_dict[chunk_idx] + (1 - alpha) * bm25_scores_normalized_dict[chunk_idx]\n",
    "        for chunk_idx in vector_scores_dict.keys()\n",
    "    }\n",
    "    \n",
    "    sorted_results = sorted(hybrid_scores_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    results = []\n",
    "    for chunk_idx, hybrid_score in sorted_results:\n",
    "        # Get metadata from collection\n",
    "        chunk_metadata = collection.get(ids=[f\"chunk_{chunk_idx}\"])['metadatas'][0]\n",
    "        results.append({\n",
    "            'chunk_idx': chunk_idx,\n",
    "            'hybrid_score': hybrid_score,\n",
    "            'vector_score': vector_scores_dict[chunk_idx],\n",
    "            'bm25_score': bm25_scores_normalized_dict[chunk_idx],\n",
    "            'content': chunks[chunk_idx],\n",
    "            'metadata': chunk_metadata\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"âœ… Hybrid retrieval function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74a8e915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LLM client initialized successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM client\n",
    "groq_client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "print(\"âœ… LLM client initialized successfully!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b7e268",
   "metadata": {},
   "source": [
    "## 1. Prompt Template Design\n",
    "\n",
    "The prompt template is the blueprint for how we combine retrieved context with the user's question. A well-designed prompt template:\n",
    "\n",
    "- **Provides clear instructions** to the LLM about its role\n",
    "- **Structures the context** in an easy-to-parse format\n",
    "- **Guides response format** (e.g., \"cite sources\", \"be concise\")\n",
    "- **Sets constraints** (e.g., \"only use provided context\")\n",
    "\n",
    "### Common Prompt Template Patterns:\n",
    "\n",
    "1. **Basic QA Template**: Simple question + context format\n",
    "2. **Instructional Template**: Detailed instructions with role definition\n",
    "3. **Citation-focused Template**: Emphasizes source attribution\n",
    "4. **Chain-of-Thought Template**: Encourages step-by-step reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d569a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Prompt templates defined!\n",
      "\n",
      "Available templates:\n",
      "1. BASIC_QA_TEMPLATE\n",
      "2. INSTRUCTIONAL_TEMPLATE\n",
      "3. CITATION_TEMPLATE\n",
      "4. CHAIN_OF_THOUGHT_TEMPLATE\n",
      "\n",
      "================================================================================\n",
      "DEMO: Prompt Template Examples\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Template 1: BASIC_QA_TEMPLATE\n",
      "--------------------------------------------------------------------------------\n",
      "Context:\n",
      "[Chunk 1]\n",
      "Remote workers must have access to reliable internet connection with minimum speeds of 25 Mbps download and 5 Mbps upload.\n",
      "\n",
      "[Chunk 2]\n",
      "All devices must have up-to-date antivirus software and firewalls enabled. The IT department provides technical support.\n",
      "\n",
      "Question: What are the internet requirements for remote work?\n",
      "\n",
      "Answer:\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Template 2: INSTRUCTIONAL_TEMPLATE\n",
      "--------------------------------------------------------------------------------\n",
      "You are a helpful assistant answering questions about remote work policies.\n",
      "\n",
      "Instructions:\n",
      "- Use ONLY the information provided in the context below\n",
      "- If the answer is not in the context, say \"I don't have enough information to answer this question\"\n",
      "- Be concise and accurate\n",
      "- Cite the relevant section when possible\n",
      "\n",
      "Context:\n",
      "[Chunk 1]\n",
      "Remote workers must have access to reliable internet connection with minimum speeds of 25 Mbps download and 5 Mbps upload.\n",
      "\n",
      "[Chunk 2]\n",
      "All devices must have up-to-date antivirus software and firewalls enabled. The IT department provides technical support.\n",
      "\n",
      "Question: What are the internet requirements for remote work?\n",
      "\n",
      "Answer:\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Template 3: CITATION_TEMPLATE\n",
      "--------------------------------------------------------------------------------\n",
      "You are a policy assistant that provides accurate answers with proper citations.\n",
      "\n",
      "Below is relevant information from our company's Remote Work Policy:\n",
      "\n",
      "[Chunk 1]\n",
      "Remote workers must have access to reliable internet connection with minimum speeds of 25 Mbps download and 5 Mbps upload.\n",
      "\n",
      "[Chunk 2]\n",
      "All devices must have up-to-date antivirus software and firewalls enabled. The IT department provides technical support.\n",
      "\n",
      "Question: What are the internet requirements for remote work?\n",
      "\n",
      "Instructions:\n",
      "- Answer the question using the information above\n",
      "- Include citations in the format [Source: chunk_X] after each fact\n",
      "- If information is not available, clearly state this\n",
      "\n",
      "Answer:\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Template 4: CHAIN_OF_THOUGHT_TEMPLATE\n",
      "--------------------------------------------------------------------------------\n",
      "You are an analytical assistant that explains your reasoning.\n",
      "\n",
      "Context Information:\n",
      "[Chunk 1]\n",
      "Remote workers must have access to reliable internet connection with minimum speeds of 25 Mbps download and 5 Mbps upload.\n",
      "\n",
      "[Chunk 2]\n",
      "All devices must have up-to-date antivirus software and firewalls enabled. The IT department provides technical support.\n",
      "\n",
      "Question: What are the internet requirements for remote work?\n",
      "\n",
      "Instructions:\n",
      "1. First, identify the relevant information from the context\n",
      "2. Explain your reasoning step-by-step\n",
      "3. Provide a clear, final answer\n",
      "\n",
      "Response:\n",
      "\n",
      "================================================================================\n",
      "ðŸ’¡ Notice how each template structures the same information differently!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Define various prompt templates\n",
    "\n",
    "# Template 1: Basic QA\n",
    "BASIC_QA_TEMPLATE = \"\"\"Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Template 2: Instructional with Role\n",
    "INSTRUCTIONAL_TEMPLATE = \"\"\"You are a helpful assistant answering questions about remote work policies.\n",
    "\n",
    "Instructions:\n",
    "- Use ONLY the information provided in the context below\n",
    "- If the answer is not in the context, say \"I don't have enough information to answer this question\"\n",
    "- Be concise and accurate\n",
    "- Cite the relevant section when possible\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Template 3: Citation-Focused\n",
    "CITATION_TEMPLATE = \"\"\"You are a policy assistant that provides accurate answers with proper citations.\n",
    "\n",
    "Below is relevant information from our company's Remote Work Policy:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Instructions:\n",
    "- Answer the question using the information above\n",
    "- Include citations in the format [Source: chunk_X] after each fact\n",
    "- If information is not available, clearly state this\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Template 4: Chain-of-Thought\n",
    "CHAIN_OF_THOUGHT_TEMPLATE = \"\"\"You are an analytical assistant that explains your reasoning.\n",
    "\n",
    "Context Information:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Instructions:\n",
    "1. First, identify the relevant information from the context\n",
    "2. Explain your reasoning step-by-step\n",
    "3. Provide a clear, final answer\n",
    "\n",
    "Response:\"\"\"\n",
    "\n",
    "print(\"âœ… Prompt templates defined!\")\n",
    "print(f\"\\nAvailable templates:\")\n",
    "print(\"1. BASIC_QA_TEMPLATE\")\n",
    "print(\"2. INSTRUCTIONAL_TEMPLATE\")\n",
    "print(\"3. CITATION_TEMPLATE\")\n",
    "print(\"4. CHAIN_OF_THOUGHT_TEMPLATE\")\n",
    "\n",
    "# Demo: Show what each template looks like with sample data\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DEMO: Prompt Template Examples\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "# Sample data for demonstration\n",
    "sample_context = \"\"\"[Chunk 1]\n",
    "Remote workers must have access to reliable internet connection with minimum speeds of 25 Mbps download and 5 Mbps upload.\n",
    "\n",
    "[Chunk 2]\n",
    "All devices must have up-to-date antivirus software and firewalls enabled. The IT department provides technical support.\"\"\"\n",
    "\n",
    "sample_query = \"What are the internet requirements for remote work?\"\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"Template 1: BASIC_QA_TEMPLATE\")\n",
    "print(\"-\" * 80)\n",
    "example1 = BASIC_QA_TEMPLATE.format(context=sample_context, query=sample_query)\n",
    "print(example1)\n",
    "print()\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"Template 2: INSTRUCTIONAL_TEMPLATE\")\n",
    "print(\"-\" * 80)\n",
    "example2 = INSTRUCTIONAL_TEMPLATE.format(context=sample_context, query=sample_query)\n",
    "print(example2)\n",
    "print()\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"Template 3: CITATION_TEMPLATE\")\n",
    "print(\"-\" * 80)\n",
    "example3 = CITATION_TEMPLATE.format(context=sample_context, query=sample_query)\n",
    "print(example3)\n",
    "print()\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"Template 4: CHAIN_OF_THOUGHT_TEMPLATE\")\n",
    "print(\"-\" * 80)\n",
    "example4 = CHAIN_OF_THOUGHT_TEMPLATE.format(context=sample_context, query=sample_query)\n",
    "print(example4)\n",
    "print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ðŸ’¡ Notice how each template structures the same information differently!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0104f9",
   "metadata": {},
   "source": [
    "## 2. Context Integration Strategies\n",
    "\n",
    "After retrieving relevant chunks, we need to decide **how** to integrate them into the prompt. Different strategies work better for different scenarios:\n",
    "\n",
    "### Strategy 1: **Simple Concatenation**\n",
    "- Join all chunks with separators\n",
    "- Fast and preserves all details\n",
    "- Risk: May exceed token limits with many chunks\n",
    "\n",
    "### Strategy 2: **Numbered/Labeled Context**\n",
    "- Add identifiers to each chunk (e.g., [Chunk 1], [Section A])\n",
    "- Enables easy citation and reference\n",
    "- Better for attribution and debugging\n",
    "\n",
    "### Strategy 3: **Metadata-Enriched Context**\n",
    "- Include metadata (source, date, topic) with each chunk\n",
    "- Provides additional context to the LLM\n",
    "- Useful for multi-document retrieval\n",
    "\n",
    "### Strategy 4: **Summarized Context**\n",
    "- Use LLM to summarize retrieved chunks before augmentation\n",
    "- Reduces token usage\n",
    "- Risk: May lose important details\n",
    "\n",
    "### Strategy 5: **Hierarchical Context**\n",
    "- Organize chunks by relevance or topic\n",
    "- Present most relevant first\n",
    "- Helps LLM prioritize information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fcb78eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Context integration strategies implemented!\n",
      "\n",
      "Available strategies:\n",
      "1. simple_concatenation()\n",
      "2. numbered_context()\n",
      "3. metadata_enriched_context()\n",
      "4. summarized_context()\n",
      "5. hierarchical_context()\n",
      "\n",
      "================================================================================\n",
      "DEMO: Context Integration Strategies\n",
      "================================================================================\n",
      "\n",
      "ðŸ” Query: 'What are the internet speed requirements?'\n",
      "ðŸ“¥ Retrieving chunks...\n",
      "\n",
      "================================================================================\n",
      "RAW RETRIEVED RESULTS (Before Context Integration)\n",
      "================================================================================\n",
      "\n",
      "Result 1:\n",
      "  Chunk ID: 3\n",
      "  Hybrid Score: 0.6077\n",
      "  Vector Score: 0.4396\n",
      "  BM25 Score: 1.0000\n",
      "  Metadata: {'chunk_index': 3, 'source': 'Remote Work Policy', 'length': 495, 'topic': 'Eligible employees must '}\n",
      "  Full Content:\n",
      "  ----------------------------------------------------------------------------\n",
      "  Eligible employees must \n",
      "demonstrate strong performance in their current role, possess excellent communication skills, and have \n",
      "access to appropriate technology and workspace. The eligibility criteria ensure that remote work benefits \n",
      "both the employee and the company. Performance reviews will be considered when evaluating eligibility. TOPIC: Technology Infrastructure\n",
      "Remote workers must have access to reliable internet connection with minimum speeds of 25 Mbps download \n",
      "and 5 Mbps upload.\n",
      "  ----------------------------------------------------------------------------\n",
      "\n",
      "Result 2:\n",
      "  Chunk ID: 4\n",
      "  Hybrid Score: 0.3640\n",
      "  Vector Score: 0.5200\n",
      "  BM25 Score: 0.0000\n",
      "  Metadata: {'topic': 'Performance reviews will be considered when evaluating eligibility. TOPIC: Technology Infrastructure', 'length': 436, 'source': 'Remote Work Policy', 'chunk_index': 4}\n",
      "  Full Content:\n",
      "  ----------------------------------------------------------------------------\n",
      "  Performance reviews will be considered when evaluating eligibility. TOPIC: Technology Infrastructure\n",
      "Remote workers must have access to reliable internet connection with minimum speeds of 25 Mbps download \n",
      "and 5 Mbps upload. Employees are responsible for maintaining secure home networks and using company-approved \n",
      "VPN software for accessing internal systems. All devices must have up-to-date antivirus software and \n",
      "firewalls enabled.\n",
      "  ----------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "ðŸ’¡ Joins chunks with separators - fast, preserves all details\n",
      "--------------------------------------------------------------------------------\n",
      "Eligible employees must \n",
      "demonstrate strong performance in their current role, possess excellent communication skills, and have \n",
      "access to appropriate technology and workspace. The eligibility criteria ensure that remote work benefits \n",
      "both the employee and the company. Performance reviews will be considered when evaluating eligibility. TOPIC: Technology Infrastructure\n",
      "Remote workers must have access to reliable internet connection with minimum speeds of 25 Mbps download \n",
      "and 5 Mbps upload.\n",
      "\n",
      "---\n",
      "\n",
      "Performance reviews will be considered when evaluating eligibility. TOPIC: Technology Infrastructure\n",
      "Remote workers must have access to reliable internet connection with minimum speeds of 25 Mbps download \n",
      "and 5 Mbps upload. Employees are responsible for maintaining secure home networks and using company-approved \n",
      "VPN software for accessing internal systems. All devices must have up-to-date antivirus software and \n",
      "firewalls enabled.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Strategy 2: Numbered Context\n",
      "ðŸ’¡ Adds [Chunk X] labels for easy citation and reference tracking\n",
      "--------------------------------------------------------------------------------\n",
      "[Chunk 3]\n",
      "Eligible employees must \n",
      "demonstrate strong performance in their current role, possess excellent communication skills, and have \n",
      "access to appropriate technology and workspace. The eligibility criteria ensure that remote work benefits \n",
      "both the employee and the company. Performance reviews will be considered when evaluating eligibility. TOPIC: Technology Infrastructure\n",
      "Remote workers must have access to reliable internet connection with minimum speeds of 25 Mbps download \n",
      "and 5 Mbps upload.\n",
      "\n",
      "[Chunk 4]\n",
      "Performance reviews will be considered when evaluating eligibility. TOPIC: Technology Infrastructure\n",
      "Remote workers must have access to reliable internet connection with minimum speeds of 25 Mbps download \n",
      "and 5 Mbps upload. Employees are responsible for maintaining secure home networks and using company-approved \n",
      "VPN software for accessing internal systems. All devices must have up-to-date antivirus software and \n",
      "firewalls enabled.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Strategy 3: Metadata-Enriched Context\n",
      "ðŸ’¡ Includes source, topic, and chunk info - provides rich context to LLM\n",
      "--------------------------------------------------------------------------------\n",
      "[Source: Remote Work Policy | Topic: Eligible employees must  | Chunk 3]\n",
      "Eligible employees must \n",
      "demonstrate strong performance in their current role, possess excellent communication skills, and have \n",
      "access to appropriate technology and workspace. The eligibility criteria ensure that remote work benefits \n",
      "both the employee and the company. Performance reviews will be considered when evaluating eligibility. TOPIC: Technology Infrastructure\n",
      "Remote workers must have access to reliable internet connection with minimum speeds of 25 Mbps download \n",
      "and 5 Mbps upload.\n",
      "\n",
      "[Source: Remote Work Policy | Topic: Performance reviews will be considered when evaluating eligibility. TOPIC: Technology Infrastructure | Chunk 4]\n",
      "Performance reviews will be considered when evaluating eligibility. TOPIC: Technology Infrastructure\n",
      "Remote workers must have access to reliable internet connection with minimum speeds of 25 Mbps download \n",
      "and 5 Mbps upload. Employees are responsible for maintaining secure home networks and using company-approved \n",
      "VPN software for accessing internal systems. All devices must have up-to-date antivirus software and \n",
      "firewalls enabled.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Strategy 4: Hierarchical Context\n",
      "ðŸ’¡ Shows relevance scores - helps LLM prioritize important information\n",
      "--------------------------------------------------------------------------------\n",
      "[Relevance: Medium | Score: 0.608]\n",
      "Eligible employees must \n",
      "demonstrate strong performance in their current role, possess excellent communication skills, and have \n",
      "access to appropriate technology and workspace. The eligibility criteria ensure that remote work benefits \n",
      "both the employee and the company. Performance reviews will be considered when evaluating eligibility. TOPIC: Technology Infrastructure\n",
      "Remote workers must have access to reliable internet connection with minimum speeds of 25 Mbps download \n",
      "and 5 Mbps upload.\n",
      "\n",
      "[Relevance: Low | Score: 0.364]\n",
      "Performance reviews will be considered when evaluating eligibility. TOPIC: Technology Infrastructure\n",
      "Remote workers must have access to reliable internet connection with minimum speeds of 25 Mbps download \n",
      "and 5 Mbps upload. Employees are responsible for maintaining secure home networks and using company-approved \n",
      "VPN software for accessing internal systems. All devices must have up-to-date antivirus software and \n",
      "firewalls enabled.\n",
      "\n",
      "================================================================================\n",
      "ðŸ’¡ Each strategy formats the same retrieved chunks differently!\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Context Integration Strategy Implementations\n",
    "\n",
    "\n",
    "def simple_concatenation(retrieved_results):\n",
    "    \"\"\"Strategy 1: Simple concatenation with separators\"\"\"\n",
    "    # Joins all chunks with separators - fast, preserves all details, but no structure\n",
    "    context = \"\\n\\n---\\n\\n\".join([result[\"content\"] for result in retrieved_results])\n",
    "    return context\n",
    "\n",
    "\n",
    "def numbered_context(retrieved_results):\n",
    "    \"\"\"Strategy 2: Add chunk numbers for citation\"\"\"\n",
    "    # Adds [Chunk X] labels to enable easy citation and reference tracking\n",
    "    context_parts = []\n",
    "    for i, result in enumerate(retrieved_results, 1):\n",
    "        context_parts.append(f\"[Chunk {result['chunk_idx']}]\\n{result['content']}\")\n",
    "    return \"\\n\\n\".join(context_parts)\n",
    "\n",
    "\n",
    "def metadata_enriched_context(retrieved_results):\n",
    "    \"\"\"Strategy 3: Include metadata with each chunk\"\"\"\n",
    "    # Adds source, topic, and chunk ID headers - provides rich context to LLM\n",
    "    context_parts = []\n",
    "    for result in retrieved_results:\n",
    "        metadata = result[\"metadata\"]\n",
    "        header = f\"[Source: {metadata['source']} | Topic: {metadata['topic']} | Chunk {result['chunk_idx']}]\"\n",
    "        context_parts.append(f\"{header}\\n{result['content']}\")\n",
    "    return \"\\n\\n\".join(context_parts)\n",
    "\n",
    "\n",
    "def hierarchical_context(retrieved_results):\n",
    "    \"\"\"Strategy 5: Organize by relevance score\"\"\"\n",
    "    # Shows relevance scores with each chunk - helps LLM prioritize information\n",
    "    context_parts = []\n",
    "    for i, result in enumerate(retrieved_results, 1):\n",
    "        relevance = (\n",
    "            \"High\"\n",
    "            if result[\"hybrid_score\"] > 0.7\n",
    "            else \"Medium\" if result[\"hybrid_score\"] > 0.5 else \"Low\"\n",
    "        )\n",
    "        header = f\"[Relevance: {relevance} | Score: {result['hybrid_score']:.3f}]\"\n",
    "        context_parts.append(f\"{header}\\n{result['content']}\")\n",
    "    return \"\\n\\n\".join(context_parts)\n",
    "\n",
    "\n",
    "def summarized_context(retrieved_results, groq_client):\n",
    "    \"\"\"Strategy 4: Summarize chunks before augmentation\"\"\"\n",
    "    # Uses LLM to compress context - reduces tokens but may lose details\n",
    "    # First concatenate all chunks\n",
    "    full_context = \"\\n\\n\".join([result[\"content\"] for result in retrieved_results])\n",
    "\n",
    "    # Ask LLM to summarize\n",
    "    summary_prompt = f\"\"\"Summarize the following text concisely, preserving key facts and details:\n",
    "\n",
    "{full_context}\n",
    "\n",
    "Summary:\"\"\"\n",
    "\n",
    "    response = groq_client.chat.completions.create(\n",
    "        model=\"llama-3.1-8b-instant\",\n",
    "        messages=[{\"role\": \"user\", \"content\": summary_prompt}],\n",
    "        temperature=0.3,\n",
    "        max_tokens=300,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "print(\"âœ… Context integration strategies implemented!\")\n",
    "print(f\"\\nAvailable strategies:\")\n",
    "print(\"1. simple_concatenation()\")\n",
    "print(\"2. numbered_context()\")\n",
    "print(\"3. metadata_enriched_context()\")\n",
    "print(\"4. summarized_context()\")\n",
    "print(\"5. hierarchical_context()\")\n",
    "\n",
    "# Demo: Test each strategy with a sample retrieval\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DEMO: Context Integration Strategies\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "sample_query = \"What are the internet speed requirements?\"\n",
    "print(f\"ðŸ” Query: '{sample_query}'\")\n",
    "print(f\"ðŸ“¥ Retrieving chunks...\\n\")\n",
    "\n",
    "sample_results = hybrid_retrieval(sample_query, top_k=2)\n",
    "\n",
    "# Show raw retrieved results BEFORE formatting\n",
    "print(\"=\" * 80)\n",
    "print(\"RAW RETRIEVED RESULTS (Before Context Integration)\")\n",
    "print(\"=\" * 80)\n",
    "for i, result in enumerate(sample_results, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"  Chunk ID: {result['chunk_idx']}\")\n",
    "    print(f\"  Hybrid Score: {result['hybrid_score']:.4f}\")\n",
    "    print(f\"  Vector Score: {result['vector_score']:.4f}\")\n",
    "    print(f\"  BM25 Score: {result['bm25_score']:.4f}\")\n",
    "    print(f\"  Metadata: {result['metadata']}\")\n",
    "    print(f\"  Full Content:\")\n",
    "    print(f\"  {'-' * 76}\")\n",
    "    print(f\"  {result['content']}\")\n",
    "    print(f\"  {'-' * 76}\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸ’¡ Joins chunks with separators - fast, preserves all details\")\n",
    "print(\"-\" * 80)\n",
    "context1 = simple_concatenation(sample_results)\n",
    "print(context1)\n",
    "print()\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"Strategy 2: Numbered Context\")\n",
    "print(\"ðŸ’¡ Adds [Chunk X] labels for easy citation and reference tracking\")\n",
    "print(\"-\" * 80)\n",
    "context2 = numbered_context(sample_results)\n",
    "print(context2)\n",
    "print()\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"Strategy 3: Metadata-Enriched Context\")\n",
    "print(\"ðŸ’¡ Includes source, topic, and chunk info - provides rich context to LLM\")\n",
    "print(\"-\" * 80)\n",
    "context3 = metadata_enriched_context(sample_results)\n",
    "print(context3)\n",
    "print()\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"Strategy 4: Hierarchical Context\")\n",
    "print(\"ðŸ’¡ Shows relevance scores - helps LLM prioritize important information\")\n",
    "print(\"-\" * 80)\n",
    "context4 = hierarchical_context(sample_results)\n",
    "print(context4)\n",
    "print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ðŸ’¡ Each strategy formats the same retrieved chunks differently!\")\n",
    "print(\"=\" * 80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448691ed",
   "metadata": {},
   "source": [
    "## 3. Complete Augmentation Pipeline\n",
    "\n",
    "Now let's build a complete augmentation function that combines:\n",
    "1. Retrieval (from Part 3)\n",
    "2. Context integration strategy\n",
    "3. Prompt template selection\n",
    "4. Final prompt generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd2cbb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Complete augmentation pipeline ready!\n"
     ]
    }
   ],
   "source": [
    "def augment_query(\n",
    "    query: str,\n",
    "    top_k: int = 3,\n",
    "    context_strategy: str = \"numbered\",\n",
    "    template_type: str = \"instructional\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete augmentation pipeline:\n",
    "    1. Retrieve relevant chunks\n",
    "    2. Apply context integration strategy\n",
    "    3. Build final prompt using template\n",
    "    \n",
    "    Args:\n",
    "        query: User's question\n",
    "        top_k: Number of chunks to retrieve\n",
    "        context_strategy: 'simple', 'numbered', 'metadata', 'hierarchical', 'summarized'\n",
    "        template_type: 'basic', 'instructional', 'citation', 'chain_of_thought'\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'prompt', 'retrieved_results', 'context'\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Retrieve relevant chunks\n",
    "    print(f\"ðŸ“¥ Retrieving top {top_k} chunks...\")\n",
    "    retrieved_results = hybrid_retrieval(query, top_k=top_k)\n",
    "    print(f\"âœ… Retrieved {len(retrieved_results)} chunks\\n\")\n",
    "    \n",
    "    # Step 2: Apply context integration strategy\n",
    "    print(f\"ðŸ”§ Applying context strategy: {context_strategy}\")\n",
    "    if context_strategy == \"simple\":\n",
    "        context = simple_concatenation(retrieved_results)\n",
    "    elif context_strategy == \"numbered\":\n",
    "        context = numbered_context(retrieved_results)\n",
    "    elif context_strategy == \"metadata\":\n",
    "        context = metadata_enriched_context(retrieved_results)\n",
    "    elif context_strategy == \"hierarchical\":\n",
    "        context = hierarchical_context(retrieved_results)\n",
    "    elif context_strategy == \"summarized\":\n",
    "        context = summarized_context(retrieved_results, groq_client)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown context strategy: {context_strategy}\")\n",
    "    \n",
    "    print(f\"âœ… Context prepared ({len(context)} chars)\\n\")\n",
    "    \n",
    "    # Step 3: Select and apply prompt template\n",
    "    print(f\"ðŸ“ Applying template: {template_type}\")\n",
    "    if template_type == \"basic\":\n",
    "        template = BASIC_QA_TEMPLATE\n",
    "    elif template_type == \"instructional\":\n",
    "        template = INSTRUCTIONAL_TEMPLATE\n",
    "    elif template_type == \"citation\":\n",
    "        template = CITATION_TEMPLATE\n",
    "    elif template_type == \"chain_of_thought\":\n",
    "        template = CHAIN_OF_THOUGHT_TEMPLATE\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown template type: {template_type}\")\n",
    "    \n",
    "    # Build final prompt\n",
    "    final_prompt = template.format(context=context, query=query)\n",
    "    print(f\"âœ… Final prompt ready ({len(final_prompt)} chars)\\n\")\n",
    "    \n",
    "    return {\n",
    "        'prompt': final_prompt,\n",
    "        'retrieved_results': retrieved_results,\n",
    "        'context': context,\n",
    "        'query': query\n",
    "    }\n",
    "\n",
    "print(\"âœ… Complete augmentation pipeline ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565859d5",
   "metadata": {},
   "source": [
    "## 4. Demo: Augmentation Pipeline in Action\n",
    "\n",
    "Let's see the complete augmentation pipeline with a practical example using numbered context and citation template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b5b49b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "AUGMENTATION DEMO: End-to-End Pipeline with Citation\n",
      "================================================================================\n",
      "\n",
      "ðŸ” User Query: 'What are the internet speed requirements for remote work?'\n",
      "\n",
      "ðŸ’¡ Why Citations Matter:\n",
      "   - Builds user trust and credibility\n",
      "   - Allows verification of information\n",
      "   - Reduces hallucinations by grounding answers in sources\n",
      "   - Enables traceability for compliance and auditing\n",
      "   - Helps users explore related information\n",
      "\n",
      "ðŸ“¥ Retrieving top 3 chunks...\n",
      "âœ… Retrieved 3 chunks\n",
      "\n",
      "ðŸ”§ Applying context strategy: numbered\n",
      "âœ… Context prepared (1523 chars)\n",
      "\n",
      "ðŸ“ Applying template: citation\n",
      "âœ… Final prompt ready (1940 chars)\n",
      "\n",
      "\n",
      "ðŸ“‹ Final Augmented Prompt:\n",
      "================================================================================\n",
      "You are a policy assistant that provides accurate answers with proper citations.\n",
      "\n",
      "Below is relevant information from our company's Remote Work Policy:\n",
      "\n",
      "[Chunk 1]\n",
      "\n",
      "Remote Work Policy\n",
      "\n",
      "TOPIC: Introduction and Benefits\n",
      "This remote work policy establishes guidelines for employees who work from home or other remote locations. Our company recognizes the benefits of remote work for both employees and the organization, including \n",
      "increased productivity, improved work-life balance, and access to a broader talent pool. These advantages \n",
      "have become increasingly important in today's dynamic work environment. TOPIC: Eligibility and Requirements\n",
      "Remote work arrangements are available to employees who meet specific criteria.\n",
      "\n",
      "[Chunk 4]\n",
      "Performance reviews will be considered when evaluating eligibility. TOPIC: Technology Infrastructure\n",
      "Remote workers must have access to reliable internet connection with minimum speeds of 25 Mbps download \n",
      "and 5 Mbps upload. Employees are responsible for maintaining secure home networks and using company-approved \n",
      "VPN software for accessing internal systems. All devices must have up-to-date antivirus software and \n",
      "firewalls enabled.\n",
      "\n",
      "[Chunk 3]\n",
      "Eligible employees must \n",
      "demonstrate strong performance in their current role, possess excellent communication skills, and have \n",
      "access to appropriate technology and workspace. The eligibility criteria ensure that remote work benefits \n",
      "both the employee and the company. Performance reviews will be considered when evaluating eligibility. TOPIC: Technology Infrastructure\n",
      "Remote workers must have access to reliable internet connection with minimum speeds of 25 Mbps download \n",
      "and 5 Mbps upload.\n",
      "\n",
      "Question: What are the internet speed requirements for remote work?\n",
      "\n",
      "Instructions:\n",
      "- Answer the question using the information above\n",
      "- Include citations in the format [Source: chunk_X] after each fact\n",
      "- If information is not available, clearly state this\n",
      "\n",
      "Answer:\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test query\n",
    "test_query = \"What are the internet speed requirements for remote work?\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"AUGMENTATION DEMO: End-to-End Pipeline with Citation\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nðŸ” User Query: '{test_query}'\\n\")\n",
    "\n",
    "print(\"ðŸ’¡ Why Citations Matter:\")\n",
    "print(\"   - Builds user trust and credibility\")\n",
    "print(\"   - Allows verification of information\")\n",
    "print(\"   - Reduces hallucinations by grounding answers in sources\")\n",
    "print(\"   - Enables traceability for compliance and auditing\")\n",
    "print(\"   - Helps users explore related information\\n\")\n",
    "\n",
    "# Use numbered context + citation template (best for attribution)\n",
    "augmented_result = augment_query(\n",
    "    query=test_query,\n",
    "    top_k=3,\n",
    "    context_strategy=\"numbered\",\n",
    "    template_type=\"citation\"\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“‹ Final Augmented Prompt:\")\n",
    "print(\"=\" * 80)\n",
    "print(augmented_result['prompt'])\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a596cf2",
   "metadata": {},
   "source": [
    "## 5. Advanced: Context Window Management\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ’¡ Note: Context Window vs. LLM Memory\n",
    "\n",
    "**Context Window** = Maximum tokens the LLM can process in one request (what we're managing here)\n",
    "\n",
    "**LLM Memory** = Conversation history across multiple turns (not covered in this notebook)\n",
    "\n",
    "In a complete RAG chatbot, your context window must fit:\n",
    "- System prompt + Retrieved chunks (this notebook) + Conversation history (memory) + Current query + Response space\n",
    "\n",
    "Context window management ensures retrieved chunks fit. Memory management (conversation history) would be an additional concern for multi-turn chatbots.\n",
    "\n",
    "### The Problem:\n",
    "Every LLM has a **context window** (maximum token limit). For example:\n",
    "- GPT-3.5-turbo: ~4K tokens\n",
    "- GPT-4: ~8K-128K tokens (depending on version)\n",
    "- Llama-3.1-8b: ~8K tokens\n",
    "\n",
    "When you retrieve many chunks or have long documents, the combined context can exceed this limit, causing:\n",
    "- API errors (request rejected)\n",
    "- Truncated context (LLM only sees partial information)\n",
    "- Increased costs (more tokens = higher price)\n",
    "\n",
    "### Real-World Challenge:\n",
    "**What if retrieved context exceeds the LLM's token limit?**\n",
    "\n",
    "You need intelligent strategies to fit context within budget while preserving the most important information.\n",
    "\n",
    "### Available Strategies:\n",
    "\n",
    "1. **Truncation (Simple & Fast)**\n",
    "   - Keep only top-N highest-scoring chunks\n",
    "   - Stops adding chunks when token limit reached\n",
    "   - âœ… Preserves most relevant information\n",
    "   - âŒ May lose valuable context from lower-ranked chunks\n",
    "\n",
    "2. **Summarization (Intelligent Compression)**\n",
    "   - Use another LLM call to compress context\n",
    "   - Condenses multiple chunks into key points\n",
    "   - âœ… Maintains semantic meaning in less space\n",
    "   - âŒ Adds latency and cost (extra LLM call)\n",
    "   - âŒ May lose specific details or nuances\n",
    "\n",
    "3. **Chunked Processing (Divide & Conquer)**\n",
    "   - Process chunks in batches, generate partial answers\n",
    "   - Merge partial answers into final response\n",
    "   - âœ… Handles very large contexts\n",
    "   - âŒ Complex implementation, multiple LLM calls\n",
    "\n",
    "4. **Smart Filtering (Preprocessing)**\n",
    "   - Remove redundant sentences using similarity\n",
    "   - Filter out low-information content\n",
    "   - âœ… Reduces noise without LLM calls\n",
    "   - âŒ Requires additional processing\n",
    "\n",
    "**In this demo, we'll implement and compare strategies #1 (Truncation) and #2 (Summarization).**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27987731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CONTEXT WINDOW MANAGEMENT DEMO\n",
      "================================================================================\n",
      "\n",
      "ðŸ’¡ Scenario: We retrieve 5 chunks, but they might exceed our token budget\n",
      "   Let's see how truncation and summarization handle this...\n",
      "\n",
      "ðŸ“ Original retrieval: 5 chunks, ~608 tokens\n",
      "\n",
      "================================================================================\n",
      "Strategy 1: Truncation (Keep top chunks until budget exhausted)\n",
      "================================================================================\n",
      "âœ‚ï¸ Truncated to 4/5 chunks\n",
      "ðŸ“Š Estimated tokens: 490/500\n",
      "ðŸ’° Token savings: 116 tokens\n",
      "\n",
      "ðŸ‘‰ Result: Kept 4 most relevant chunks\n",
      "   Use case: When you want to preserve exact wording of top chunks\n",
      "\n",
      "================================================================================\n",
      "Strategy 2: Summarization (Compress context using LLM)\n",
      "================================================================================\n",
      "ðŸ“‰ Context too large (608 tokens > 500 limit)\n",
      "ðŸ¤– Calling LLM to summarize...\n",
      "âœ… Summarized to 218 tokens (reduction: 390 tokens)\n",
      "\n",
      "ðŸ‘‰ Result: 1 chunk(s) - Summary\n",
      "   Use case: When you need to fit more information in less space\n",
      "\n",
      "================================================================================\n",
      "ðŸŽ¯ Key Takeaway:\n",
      "   - Truncation: Fast, preserves details, but discards lower-ranked chunks\n",
      "   - Summarization: Compresses all info, but adds latency and may lose details\n",
      "   - Choose based on: token budget, latency requirements, and detail importance\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TOKEN ESTIMATION & CONTEXT WINDOW MANAGEMENT\n",
    "# ============================================================================\n",
    "\n",
    "def estimate_tokens(text):\n",
    "    \"\"\"\n",
    "    Estimate the number of tokens in text.\n",
    "    \n",
    "    This is a rough approximation. Real tokenization depends on the specific\n",
    "    tokenizer used by the LLM (e.g., tiktoken for OpenAI models).\n",
    "    \n",
    "    Rule of thumb for English:\n",
    "    - 1 token â‰ˆ 4 characters\n",
    "    - 1 token â‰ˆ 0.75 words\n",
    "    - 100 tokens â‰ˆ 75 words\n",
    "    \n",
    "    For production use, consider using actual tokenizers:\n",
    "    - OpenAI: tiktoken library\n",
    "    - Hugging Face: transformers.AutoTokenizer\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to estimate tokens for\n",
    "        \n",
    "    Returns:\n",
    "        Estimated token count (int)\n",
    "    \"\"\"\n",
    "    return len(text) // 4\n",
    "\n",
    "\n",
    "def manage_context_window(\n",
    "    retrieved_results,\n",
    "    max_tokens=2000,\n",
    "    strategy=\"truncate\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Intelligently manage context to fit within LLM token budget.\n",
    "    \n",
    "    This function implements two strategies for handling contexts that may\n",
    "    exceed the LLM's maximum token limit:\n",
    "    \n",
    "    1. TRUNCATION: Greedily add chunks (sorted by relevance) until budget exhausted\n",
    "       - Fast and simple\n",
    "       - Preserves most relevant chunks\n",
    "       - No additional LLM calls\n",
    "       \n",
    "    2. SUMMARIZATION: Use LLM to compress context into shorter form\n",
    "       - Better semantic preservation\n",
    "       - Requires extra LLM call (adds latency + cost)\n",
    "       - May lose specific details\n",
    "    \n",
    "    Args:\n",
    "        retrieved_results: List of dicts with 'content', 'chunk_idx', \n",
    "                          'hybrid_score', and 'metadata' keys\n",
    "        max_tokens: Maximum tokens allowed for context (reserve space for \n",
    "                   prompt template and LLM response)\n",
    "        strategy: 'truncate' or 'summarize'\n",
    "        \n",
    "    Returns:\n",
    "        List of chunk dicts that fit within token budget\n",
    "        \n",
    "    Example:\n",
    "        >>> chunks = hybrid_retrieval(\"query\", top_k=10)\n",
    "        >>> managed = manage_context_window(chunks, max_tokens=2000, strategy=\"truncate\")\n",
    "        >>> # managed now contains only chunks that fit in 2000 tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    if strategy == \"truncate\":\n",
    "        # STRATEGY 1: TRUNCATION\n",
    "        # Iterate through chunks (already sorted by relevance) and add them\n",
    "        # until we hit the token budget limit\n",
    "        \n",
    "        selected = []  # Chunks that fit within budget\n",
    "        current_tokens = 0  # Running token count\n",
    "        \n",
    "        for result in retrieved_results:\n",
    "            # Estimate tokens for this chunk\n",
    "            chunk_tokens = estimate_tokens(result['content'])\n",
    "            \n",
    "            # Check if adding this chunk would exceed budget\n",
    "            if current_tokens + chunk_tokens <= max_tokens:\n",
    "                selected.append(result)\n",
    "                current_tokens += chunk_tokens\n",
    "            else:\n",
    "                # Budget exhausted, stop adding chunks\n",
    "                # Note: This means lower-ranked chunks are discarded\n",
    "                break\n",
    "        \n",
    "        print(f\"âœ‚ï¸ Truncated to {len(selected)}/{len(retrieved_results)} chunks\")\n",
    "        print(f\"ðŸ“Š Estimated tokens: {current_tokens}/{max_tokens}\")\n",
    "        print(f\"ðŸ’° Token savings: {estimate_tokens(''.join([r['content'] for r in retrieved_results])) - current_tokens} tokens\")\n",
    "        return selected\n",
    "    \n",
    "    elif strategy == \"summarize\":\n",
    "        # STRATEGY 2: SUMMARIZATION\n",
    "        # First check if context exceeds budget, then use LLM to compress\n",
    "        \n",
    "        # Combine all chunks into single text\n",
    "        full_text = \"\\n\\n\".join([r['content'] for r in retrieved_results])\n",
    "        total_tokens = estimate_tokens(full_text)\n",
    "        \n",
    "        if total_tokens > max_tokens:\n",
    "            # Context too large - need to summarize\n",
    "            print(f\"ðŸ“‰ Context too large ({total_tokens} tokens > {max_tokens} limit)\")\n",
    "            print(f\"ðŸ¤– Calling LLM to summarize...\")\n",
    "            \n",
    "            # Use the summarized_context function (defined earlier) to compress\n",
    "            # This makes an LLM call to condense the context\n",
    "            summary = summarized_context(retrieved_results, groq_client)\n",
    "            summary_tokens = estimate_tokens(summary)\n",
    "            \n",
    "            print(f\"âœ… Summarized to {summary_tokens} tokens (reduction: {total_tokens - summary_tokens} tokens)\")\n",
    "            \n",
    "            # Return as a pseudo-chunk with special metadata\n",
    "            return [{\n",
    "                'chunk_idx': -1,  # Special ID indicating this is a summary\n",
    "                'content': summary,\n",
    "                'hybrid_score': 1.0,  # Max score since it's a summary of all\n",
    "                'metadata': {\n",
    "                    'source': 'Summary',\n",
    "                    'topic': 'Consolidated',\n",
    "                    'original_chunks': len(retrieved_results),\n",
    "                    'original_tokens': total_tokens\n",
    "                }\n",
    "            }]\n",
    "        else:\n",
    "            # Context already fits within budget - no action needed\n",
    "            print(f\"âœ… Context within budget ({total_tokens}/{max_tokens} tokens)\")\n",
    "            return retrieved_results\n",
    "    \n",
    "    # Fallback: return original results if strategy not recognized\n",
    "    return retrieved_results\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DEMONSTRATION: Context Window Management\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONTEXT WINDOW MANAGEMENT DEMO\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "print(\"ðŸ’¡ Scenario: We retrieve 5 chunks, but they might exceed our token budget\")\n",
    "print(\"   Let's see how truncation and summarization handle this...\\n\")\n",
    "\n",
    "# Retrieve more chunks than usual to demonstrate the problem\n",
    "large_retrieval = hybrid_retrieval(test_query, top_k=5)\n",
    "\n",
    "# Show original context size\n",
    "original_text = \"\\n\\n\".join([r['content'] for r in large_retrieval])\n",
    "original_tokens = estimate_tokens(original_text)\n",
    "print(f\"ðŸ“ Original retrieval: {len(large_retrieval)} chunks, ~{original_tokens} tokens\\n\")\n",
    "\n",
    "# -------------------------\n",
    "# STRATEGY 1: TRUNCATION\n",
    "# -------------------------\n",
    "print(\"=\" * 80)\n",
    "print(\"Strategy 1: Truncation (Keep top chunks until budget exhausted)\")\n",
    "print(\"=\" * 80)\n",
    "managed_truncate = manage_context_window(large_retrieval, max_tokens=500, strategy=\"truncate\")\n",
    "print(f\"\\nðŸ‘‰ Result: Kept {len(managed_truncate)} most relevant chunks\")\n",
    "print(\"   Use case: When you want to preserve exact wording of top chunks\\n\")\n",
    "\n",
    "# -------------------------\n",
    "# STRATEGY 2: SUMMARIZATION\n",
    "# -------------------------\n",
    "print(\"=\" * 80)\n",
    "print(\"Strategy 2: Summarization (Compress context using LLM)\")\n",
    "print(\"=\" * 80)\n",
    "managed_summarize = manage_context_window(large_retrieval, max_tokens=500, strategy=\"summarize\")\n",
    "print(f\"\\nðŸ‘‰ Result: {len(managed_summarize)} chunk(s) - {'Summary' if managed_summarize[0]['chunk_idx'] == -1 else 'Original'}\")\n",
    "print(\"   Use case: When you need to fit more information in less space\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸŽ¯ Key Takeaway:\")\n",
    "print(\"   - Truncation: Fast, preserves details, but discards lower-ranked chunks\")\n",
    "print(\"   - Summarization: Compresses all info, but adds latency and may lose details\")\n",
    "print(\"   - Choose based on: token budget, latency requirements, and detail importance\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789a0304",
   "metadata": {},
   "source": [
    "## 6. Comparison: Different Augmentation Approaches\n",
    "\n",
    "Let's compare how different augmentation strategies affect the final answer quality. We'll briefly use generation here for comparison purposes only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aeab882c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPARATIVE ANALYSIS: AUGMENTATION STRATEGIES\n",
      "================================================================================\n",
      "\n",
      "ðŸ” Query: 'What are the internet speed requirements for remote work?'\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Testing: Simple + Basic\n",
      "================================================================================\n",
      "ðŸ“¥ Retrieving top 3 chunks...\n",
      "âœ… Retrieved 3 chunks\n",
      "\n",
      "ðŸ”§ Applying context strategy: simple\n",
      "âœ… Context prepared (1503 chars)\n",
      "\n",
      "ðŸ“ Applying template: basic\n",
      "âœ… Final prompt ready (1590 chars)\n",
      "\n",
      "ðŸ¤– Generating answer...\n",
      "\n",
      "ðŸ“ Answer (100 chars):\n",
      "--------------------------------------------------------------------------------\n",
      "The internet speed requirements for remote work are a minimum of 25 Mbps download and 5 Mbps upload.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "Testing: Numbered + Citation\n",
      "================================================================================\n",
      "ðŸ“¥ Retrieving top 3 chunks...\n",
      "âœ… Retrieved 3 chunks\n",
      "\n",
      "ðŸ”§ Applying context strategy: numbered\n",
      "âœ… Context prepared (1523 chars)\n",
      "\n",
      "ðŸ“ Applying template: citation\n",
      "âœ… Final prompt ready (1940 chars)\n",
      "\n",
      "ðŸ¤– Generating answer...\n",
      "\n",
      "ðŸ“ Answer (442 chars):\n",
      "--------------------------------------------------------------------------------\n",
      "According to our company's Remote Work Policy, remote workers must have access to a reliable internet connection with minimum speeds of 25 Mbps download and 5 Mbps upload. \n",
      "\n",
      "[Source: Chunk 4]\n",
      "\n",
      "Additionally, this requirement is also mentioned in Chunk 3, which states the same internet speed requirements for remote workers.\n",
      "\n",
      "[Source: Chunk 3]\n",
      "\n",
      "Therefore, the internet speed requirements for remote work are 25 Mbps download and 5 Mbps upload.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "Testing: Metadata + Instructional\n",
      "================================================================================\n",
      "ðŸ“¥ Retrieving top 3 chunks...\n",
      "âœ… Retrieved 3 chunks\n",
      "\n",
      "ðŸ”§ Applying context strategy: metadata\n",
      "âœ… Context prepared (1764 chars)\n",
      "\n",
      "ðŸ“ Applying template: instructional\n",
      "âœ… Final prompt ready (2169 chars)\n",
      "\n",
      "ðŸ¤– Generating answer...\n",
      "\n",
      "ðŸ“ Answer (196 chars):\n",
      "--------------------------------------------------------------------------------\n",
      "According to the Remote Work Policy, remote workers must have access to a reliable internet connection with minimum speeds of 25 Mbps download and 5 Mbps upload. (Topic: Technology Infrastructure)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "COMPARISON SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Strategy                           Prompt    Context     Answer\n",
      "--------------------------------------------------------------------------------\n",
      "Simple + Basic                       1590       1503        100\n",
      "Numbered + Citation                  1940       1523        442\n",
      "Metadata + Instructional             2169       1764        196\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ðŸ’¡ Insights:\n",
      "   - Citation template produces answers with source attribution\n",
      "   - Metadata enrichment provides more context to the LLM\n",
      "   - Simple strategies are faster but may lack structure\n",
      "   - Choose based on your use case: speed vs. attribution vs. detail\n"
     ]
    }
   ],
   "source": [
    "# Helper function for generation (used only for comparison)\n",
    "def generate_answer(augmented_result, model=\"llama-3.1-8b-instant\", temperature=0.3):\n",
    "    \"\"\"Generate answer using LLM with augmented prompt.\"\"\"\n",
    "    response = groq_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": augmented_result['prompt']}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARATIVE ANALYSIS: AUGMENTATION STRATEGIES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_queries = [\n",
    "    \"What are the internet speed requirements for remote work?\",\n",
    "    \"How is remote work performance evaluated?\",\n",
    "    \"What communication tools should remote workers use?\"\n",
    "]\n",
    "\n",
    "strategies = [\n",
    "    (\"simple\", \"basic\", \"Simple + Basic\"),\n",
    "    (\"numbered\", \"citation\", \"Numbered + Citation\"),\n",
    "    (\"metadata\", \"instructional\", \"Metadata + Instructional\"),\n",
    "]\n",
    "\n",
    "# Test first query with all strategies\n",
    "query = test_queries[0]\n",
    "print(f\"\\nðŸ” Query: '{query}'\\n\")\n",
    "\n",
    "results_comparison = []\n",
    "\n",
    "for context_strat, template_type, label in strategies:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing: {label}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Augment\n",
    "    augmented = augment_query(\n",
    "        query=query,\n",
    "        top_k=3,\n",
    "        context_strategy=context_strat,\n",
    "        template_type=template_type\n",
    "    )\n",
    "    \n",
    "    # Generate\n",
    "    print(\"ðŸ¤– Generating answer...\")\n",
    "    answer = generate_answer(augmented, temperature=0.2)  # Low temp for consistency\n",
    "    \n",
    "    # Calculate metrics\n",
    "    prompt_length = len(augmented['prompt'])\n",
    "    answer_length = len(answer)\n",
    "    context_length = len(augmented['context'])\n",
    "    \n",
    "    results_comparison.append({\n",
    "        'strategy': label,\n",
    "        'answer': answer,\n",
    "        'prompt_length': prompt_length,\n",
    "        'answer_length': answer_length,\n",
    "        'context_length': context_length\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nðŸ“ Answer ({answer_length} chars):\")\n",
    "    print(\"-\" * 80)\n",
    "    print(answer)\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n{'Strategy':<30} {'Prompt':>10} {'Context':>10} {'Answer':>10}\")\n",
    "print(\"-\" * 80)\n",
    "for r in results_comparison:\n",
    "    print(f\"{r['strategy']:<30} {r['prompt_length']:>10} {r['context_length']:>10} {r['answer_length']:>10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\\nðŸ’¡ Insights:\")\n",
    "print(\"   - Citation template produces answers with source attribution\")\n",
    "print(\"   - Metadata enrichment provides more context to the LLM\")\n",
    "print(\"   - Simple strategies are faster but may lack structure\")\n",
    "print(\"   - Choose based on your use case: speed vs. attribution vs. detail\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b447778e",
   "metadata": {},
   "source": [
    "## 7. Best Practices & Recommendations\n",
    "\n",
    "### âœ… Key Takeaways:\n",
    "\n",
    "#### **Prompt Template Selection**\n",
    "- **For customer support**: Use instructional template with clear constraints\n",
    "- **For research/analysis**: Use chain-of-thought template\n",
    "- **For compliance/legal**: Use citation template with strict source requirements\n",
    "- **For chatbots**: Use conversational templates with personality\n",
    "\n",
    "#### **Context Integration Strategy**\n",
    "- **Default choice**: Numbered context (enables citations, minimal overhead)\n",
    "- **When context is large**: Use summarized context or truncation\n",
    "- **Multi-document scenarios**: Use metadata-enriched context\n",
    "- **High-precision tasks**: Use hierarchical context (relevance-sorted)\n",
    "\n",
    "#### **Token Budget Management**\n",
    "- Always estimate token usage before sending to LLM\n",
    "- Reserve 20-30% of context window for LLM response\n",
    "- Consider using smaller models for summarization step\n",
    "- Implement graceful fallbacks if context is too large\n",
    "\n",
    "#### **Citation & Trust**\n",
    "- Always include source identifiers in context\n",
    "- Teach LLM to cite sources through examples and instructions\n",
    "- Extract and validate citations programmatically\n",
    "- Consider adding confidence scores to citations\n",
    "\n",
    "#### **Testing & Iteration**\n",
    "- Test different strategies on representative queries\n",
    "- Measure: answer quality, citation accuracy, token usage\n",
    "- A/B test with real users when possible\n",
    "- Monitor for hallucinations (answers not grounded in context)\n",
    "\n",
    "### ðŸ”„ Recommended Pipeline:\n",
    "\n",
    "```\n",
    "1. Retrieve (hybrid search, top-5)\n",
    "   â†“\n",
    "2. Rerank (cross-encoder, keep top-3)\n",
    "   â†“\n",
    "3. Context Integration (numbered)\n",
    "   â†“\n",
    "4. Prompt Template (citation-focused)\n",
    "   â†“\n",
    "5. Generate (with citations)\n",
    "   â†“\n",
    "6. Validate Citations\n",
    "   â†“\n",
    "7. Return Answer + Sources\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
